{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "\n",
    "import os\n",
    "import urllib, cStringIO\n",
    "\n",
    "import pymongo as pm\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting up paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('../..')\n",
    "analysis_dir = os.getcwd()\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "features_dir= os.path.join(results_dir,'features')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'experiments'))\n",
    "sketch_dir = os.path.abspath(os.path.join(proj_dir,'sketches'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'analysis'))\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)  \n",
    "\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(features_dir):\n",
    "    os.makedirs(features_dir)\n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'analysis'))        \n",
    "    \n",
    "# Assign variables within imported analysis helpers\n",
    "import analysis_helpers as h\n",
    "if sys.version_info[0]>=3:\n",
    "    from importlib import reload\n",
    "reload(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_df(X):\n",
    "    if 'Unnamed: 0' in X.columns:\n",
    "        X = X.drop(columns=['Unnamed: 0'])\n",
    "    return X\n",
    "\n",
    "def flatten(x):\n",
    "    return [item for sublist in x for item in sublist]\n",
    "\n",
    "def normalize(X):\n",
    "    X = X - X.mean(0)\n",
    "    X = X / np.maximum(X.std(0), 1e-5)\n",
    "    return X\n",
    "\n",
    "def get_ordered_objs_list_by_category(F):\n",
    "    objs_list = []\n",
    "    close_inds = F['condition'] == 'closer'\n",
    "    far_inds = F['condition'] == 'further'\n",
    "    categories = ['bird','car','chair','dog']\n",
    "    for this_category in categories:\n",
    "        category_inds = F['category'] == this_category\n",
    "        objs_list.append(list(F[(category_inds) & (far_inds)].reset_index(drop=True).target.values))\n",
    "    return flatten(objs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## helper dictionaries \n",
    "OBJECT_TO_CATEGORY = {\n",
    "    'basset': 'dog', 'beetle': 'car', 'bloodhound': 'dog', 'bluejay': 'bird',\n",
    "    'bluesedan': 'car', 'bluesport': 'car', 'brown': 'car', 'bullmastiff': 'dog',\n",
    "    'chihuahua': 'dog', 'crow': 'bird', 'cuckoo': 'bird', 'doberman': 'dog',\n",
    "    'goldenretriever': 'dog', 'hatchback': 'car', 'inlay': 'chair', 'knob': 'chair',\n",
    "    'leather': 'chair', 'nightingale': 'bird', 'pigeon': 'bird', 'pug': 'dog',\n",
    "    'redantique': 'car', 'redsport': 'car', 'robin': 'bird', 'sling': 'chair',\n",
    "    'sparrow': 'bird', 'squat': 'chair', 'straight': 'chair', 'tomtit': 'bird',\n",
    "    'waiting': 'chair', 'weimaraner': 'dog', 'white': 'car', 'woven': 'chair',\n",
    "}\n",
    "CATEGORY_TO_OBJECT = {\n",
    "    'dog': ['basset', 'bloodhound', 'bullmastiff', 'chihuahua', 'doberman', 'goldenretriever', 'pug', 'weimaraner'],\n",
    "    'car': ['beetle', 'bluesedan', 'bluesport', 'brown', 'hatchback', 'redantique', 'redsport', 'white'],\n",
    "    'bird': ['bluejay', 'crow', 'cuckoo', 'nightingale', 'pigeon', 'robin', 'sparrow', 'tomtit'],\n",
    "    'chair': ['inlay', 'knob', 'leather', 'sling', 'squat', 'straight', 'waiting', 'woven'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## loading in files we need\n",
    "dataset = 'rawcounts'\n",
    "features = cleanup_df(pd.read_csv(os.path.join(features_dir,'semantic_parts_sketch_features_compressed_{}.csv'.format(dataset))))\n",
    "meta = cleanup_df(pd.read_csv(os.path.join(features_dir,'semantic_parts_sketch_meta.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## sanity check: make sure that the numstrokes and arclength features each add up to 1\n",
    "numstrokes_cols = [i for i in feature_df.columns if i.split('_')[-1]=='numstrokes']\n",
    "arclength_cols = [i for i in feature_df.columns if i.split('_')[-1]=='arclength']\n",
    "if dataset=='normalized':\n",
    "    assert len(np.unique(feature_df[arclength_cols].sum(axis=1).round(10)))==1\n",
    "    assert len(np.unique(feature_df[numstrokes_cols].sum(axis=1).round(10)))==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## normalize feature_df (apply channel_normalization)? \n",
    "## Warning, this will make it so numstrokes and arclength features DO NOT add up to 1\n",
    "channel_normalization = True\n",
    "if channel_normalization:\n",
    "    feature_df = normalize(feature_df)\n",
    "    print 'Applied channel_normalization to raw feature matrix.'\n",
    "else:\n",
    "    print 'Did not apply channel_normalization to raw feature matrix.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## concatenate meta and features to enable easy subsetting of dataframe\n",
    "F = pd.concat((meta,features),axis=1)\n",
    "\n",
    "## add category to F dataframe so we can subset on that later\n",
    "F['category'] = F['target'].apply(lambda x: OBJECT_TO_CATEGORY[x])\n",
    "\n",
    "# hacky way of guarding against accidentally over-writing F, have a copy here called F0\n",
    "F0 = F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate by (object, context-condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## aggregate by target and condition and take the mean across rows within each group\n",
    "F2 = F.groupby(['target','condition']).mean().reset_index()\n",
    "\n",
    "## re-add category back to the F dataframe so we can subset on that later \n",
    "##( taking mean above removes it b/c it is a string)\n",
    "F2['category'] = F2['target'].apply(lambda x: OBJECT_TO_CATEGORY[x])\n",
    "\n",
    "## get ordered list of all objects\n",
    "obj_list = np.unique(F.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subset by context condition and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get names of columns that contain stroke-count & arclength information\n",
    "numstrokes_cols = [i for i in feature_df.columns if i.split('_')[-1]=='numstrokes']\n",
    "arclength_cols = [i for i in feature_df.columns if i.split('_')[-1]=='arclength']\n",
    "feat_cols = numstrokes_cols + arclength_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define scope of comparison\n",
    "def subset_dataframe_by_condition(F,to_inspect='all',this_category='dog',this_object='pug'):\n",
    "    '''\n",
    "    input: F: dataframe (num_sketches x num_features)\n",
    "           to_inspect: a string indicating whether to subset by ['object','category','all']\n",
    "           this_category: IF to_inspect == 'category', then we define this to subset by that category only\n",
    "           this_object: IF to_inspect == 'object', then we define this to subset by that object only\n",
    "           \n",
    "    returns: two feature matrices, c and f, corresponding to the close and far subsetted feature matrices\n",
    "           \n",
    "    '''\n",
    "        \n",
    "    ## ADD THIS LINE: sort F by category and object\n",
    "    F = F.sort_values(by=['category','target'])\n",
    "\n",
    "    ## get context condition inds for subsetting dataframe\n",
    "    close_inds = F['condition'] == 'closer'\n",
    "    far_inds = F['condition'] == 'further'\n",
    "\n",
    "    ## if we want to inspect particular category\n",
    "    category_inds = F['category']==this_category\n",
    "\n",
    "    ## if we want to inspect particular object\n",
    "    obj_list = np.unique(F.target.values)\n",
    "    obj_inds = F['target']==this_object  \n",
    "    \n",
    "    ## get names of columns that contain stroke-count & arclength information\n",
    "    numstrokes_cols = [i for i in F.columns if i.split('_')[-1]=='numstrokes']\n",
    "    arclength_cols = [i for i in F.columns if i.split('_')[-1]=='arclength']\n",
    "    feat_cols = numstrokes_cols + arclength_cols\n",
    "    \n",
    "    if to_inspect == 'object':    \n",
    "        ## extract particular row corresponding to this OBJECT in each condition\n",
    "        f = F[(far_inds) & obj_inds][feat_cols].reset_index(drop=True)\n",
    "        c = F[(close_inds) & obj_inds][feat_cols].reset_index(drop=True)\n",
    "        obj_list = F[(far_inds) & obj_inds]['target'].values\n",
    "    elif to_inspect == 'category':\n",
    "        ## extract particular rows corresponding to this CATEGORY in each condition\n",
    "        f = F[(category_inds) & (far_inds)][feat_cols].reset_index(drop=True)\n",
    "        c = F[(category_inds) & (close_inds)][feat_cols].reset_index(drop=True)\n",
    "        obj_list = F[(category_inds) & (far_inds)]['target'].values\n",
    "    elif to_inspect == 'all':\n",
    "        ## extract particular rows corresponding to each condition\n",
    "        f = F[far_inds][feat_cols].reset_index(drop=True)\n",
    "        c = F[close_inds][feat_cols].reset_index(drop=True) \n",
    "        obj_list = F[far_inds]['target'].values\n",
    "    return c, f, obj_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing close vs. far sketch dispersion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_inspect = 'category'\n",
    "this_category = 'dog'\n",
    "c,f, obj_list = subset_dataframe_by_condition(F2,\n",
    "                                    to_inspect=to_inspect,\n",
    "                                    this_category=this_category)\n",
    "\n",
    "print 'Order of objects in subset is {}.'.format(obj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## stack the close and far feature matrices to get (16 x K) feature matrix\n",
    "fmat = np.vstack((np.array(c),np.array(f)))\n",
    "\n",
    "## sanity check to make sure there are a total of 16 rows b/c there are 8 objects x 2 context conditions \n",
    "if to_inspect=='category':\n",
    "    assert np.vstack((np.array(c),np.array(f))).shape[0] == 16\n",
    "elif to_inspect=='all':\n",
    "    assert np.vstack((np.array(c),np.array(f))).shape[0] == 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## produce visualization of matrix\n",
    "from sklearn.metrics import *\n",
    "sns.set_style('white')\n",
    "D = pairwise_distances(fmat,metric='euclidean')\n",
    "plt.matshow(D)\n",
    "plt.plot((7.5, 7.5), (-0.5, 15.5), 'k-') # vertical refline\n",
    "plt.plot((-0.5, 15.5), (7.5, 7.5), 'k-') # horizontal refline\n",
    "plt.colorbar(fraction=0.05)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('close          far')\n",
    "plt.ylabel('far          close')\n",
    "plt.title('object distances x condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Refresher on properties of variance\n",
    "https://en.wikipedia.org/wiki/Variance\n",
    "\n",
    "$ \\operatorname{Var}(X) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2 $\n",
    "\n",
    "$ \\operatorname{Var}(X) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\frac{1}{2}(x_i - x_j)^2 = \\frac{1}{n^2}\\sum_i \\sum_{j>i} (x_i-x_j)^2. $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## for each category, compare the within-condition variance \n",
    "## and the length of the centroid vector \n",
    "to_inspect = 'category'\n",
    "dist_metric = 'euclidean'\n",
    "categories = ['bird','car','chair','dog']\n",
    "\n",
    "for i, this_category in enumerate(categories):\n",
    "    c,f,obj_list = subset_dataframe_by_condition(F2,\n",
    "                                        to_inspect=to_inspect,\n",
    "                                        this_category=this_category) ## get subset of features\n",
    "\n",
    "    fmat = np.vstack((np.array(c),np.array(f))) ## stack\n",
    "    D = pairwise_distances(fmat,metric=dist_metric) ## get distances\n",
    "    dim = D.shape[0]\n",
    "    half_dim = int(dim/2)\n",
    "    triu_inds = np.triu_indices(half_dim,k=1)\n",
    "\n",
    "    ## get pairwise distances between objects within context condition\n",
    "    close_pairwise_dists = D[:half_dim,:half_dim][triu_inds]\n",
    "    far_pairwise_dists = D[half_dim:dim,half_dim:dim][triu_inds]\n",
    "\n",
    "\n",
    "    ## get euclidean distances of each object from origin \n",
    "    frob_c = np.apply_along_axis(np.linalg.norm,1,c)\n",
    "    frob_f = np.apply_along_axis(np.linalg.norm,1,f)\n",
    "\n",
    "    ## get std and mean distance from zero for each condition\n",
    "    far_std = np.mean(far_pairwise_dists)\n",
    "    far_mean = np.mean(frob_f)\n",
    "\n",
    "    close_std = np.mean(close_pairwise_dists)\n",
    "    close_mean = np.mean(frob_c)\n",
    "\n",
    "    ## compute coefficient of variation, \n",
    "    ## measure of what percentage of the mean the dispersion is ...co\n",
    "    ## if low, indicates higher mean relative to variance\n",
    "    ## if high, indicates higher variance relative to mean\n",
    "    ## https://en.wikipedia.org/wiki/Coefficient_of_variation\n",
    "    far_cv = far_std/far_mean\n",
    "    close_cv = close_std/close_mean\n",
    "    print close_std, far_std\n",
    "    print 'Category: {} | CV for close: {}, CV for far: {}.'.format(this_category, close_cv.round(3), far_cv.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lower CV for far indicates tighter distribution, even accounting for distance from origin.\n",
    "    That is, far sketches are more similar to each other than close sketches are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ##Some sanity checks for how CV scales\n",
    "\n",
    "# c_mock = np.random.randint(40,80,(32,48))\n",
    "# f_mock = np.random.randint(0,50,(32,48))\n",
    "# ## create a mock version of the close matrix, with component values double that of the normal\n",
    "# c_double = c_mock*2\n",
    "\n",
    "# fmat = np.vstack((c_mock,f_mock)) ## stack\n",
    "# D = pairwise_distances(fmat,metric=dist_metric) ## get distances\n",
    "# dim = D.shape[0]\n",
    "# half_dim = int(dim/2)\n",
    "# triu_inds = np.triu_indices(half_dim,k=1)\n",
    "\n",
    "\n",
    "# #make mock Distance matrix with mock close matrix values\n",
    "# D_mock = pairwise_distances(np.vstack((c_double,f_mock)),metric=dist_metric)\n",
    "\n",
    "\n",
    "\n",
    "# ## get pairwise distances between objects within context condition\n",
    "# close_pairwise_dists = D[:half_dim,:half_dim][triu_inds]\n",
    "\n",
    "# far_pairwise_dists = D[half_dim:dim,half_dim:dim][triu_inds]\n",
    "\n",
    "# c_double_close_pairwise_dists= D_mock[:half_dim,:half_dim][triu_inds]\n",
    "# c_double_far_pairwise_dists= D_mock[half_dim:dim,half_dim:dim][triu_inds]\n",
    "\n",
    "# ## get euclidean distances of each object from origin \n",
    "# frob_c = np.apply_along_axis(np.linalg.norm,1,c_mock)\n",
    "# frob_f = np.apply_along_axis(np.linalg.norm,1,f_mock)\n",
    "\n",
    "# ##euclidean distance of mock vectors\n",
    "# frob_c_double = np.apply_along_axis(np.linalg.norm,1,c_double)\n",
    "\n",
    "# ## get std and mean distance from zero for each condition\n",
    "# far_std = np.mean(far_pairwise_dists)\n",
    "# far_mean = np.mean(frob_f)\n",
    "\n",
    "# close_std = np.mean(close_pairwise_dists)\n",
    "# close_mean = np.mean(frob_c)\n",
    "\n",
    "# c_double_close_std= np.mean(c_double_close_pairwise_dists)\n",
    "# c_double_close_mean = np.mean(c_double)\n",
    "\n",
    "# c_double_far_std = np.mean(c_double_far_pairwise_dists)\n",
    "# c_double_far_mean = np.mean(frob_f)\n",
    "# ## compute coefficient of variation, \n",
    "# ## measure of what percentage of the mean the dispersion is ...co\n",
    "# ## if low, indicates higher mean relative to variance\n",
    "# ## if high, indicates higher variance relative to mean\n",
    "# ## https://en.wikipedia.org/wiki/Coefficient_of_variation\n",
    "# far_cv = far_std/far_mean\n",
    "# close_cv = close_std/close_mean\n",
    "# double_c_close_cv = c_double_close_std/c_double_close_mean\n",
    "# double_c_far_cv = c_double_far_std/c_double_far_mean\n",
    "\n",
    "\n",
    "# print ' CV for close: {}, CV for far: {},\\\n",
    "#  mock CV close:{}, mock CV far: {}'.format(far_cv.round(4), close_cv.round(4), double_c_close_cv.round(4), \\\n",
    "#                                            double_c_far_cv.round(4))\n",
    "\n",
    "# print 'close-far CV ratio for no manipulation  : {}, close-far CV ratio for double c : {}'.format((far_cv/close_cv).round(4),\\\n",
    "#                                                                                               (double_c_far_cv/double_c_close_cv).round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking how CV scales with vector magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_mock = np.random.randint(0,100,(32,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Some sanity checks for how CV scales| Change the multiplicand 'double' for different multiples of the mock vector\n",
    "#'CLOSE' here is simply double the value of the mock \"FAR\" vector\n",
    "dist_metric = 'euclidean'\n",
    "f_double = f_mock*5\n",
    "\n",
    "fmat = np.vstack((f_double,f_mock)) ## stacking close on top of far\n",
    "D_mock = pairwise_distances(fmat,metric=dist_metric) ## getting distances\n",
    "dim = D.shape[0]\n",
    "half_dim = int(dim/2)\n",
    "triu_inds = np.triu_indices(half_dim,k=1)\n",
    "\n",
    "\n",
    "## get pairwise distances between objects within context condition\n",
    "close_pairwise_dists = D_mock[:half_dim,:half_dim][triu_inds]\n",
    "\n",
    "far_pairwise_dists = D_mock[half_dim:dim,half_dim:dim][triu_inds]\n",
    "\n",
    "## get euclidean distances of each object from origin \n",
    "frob_f = np.apply_along_axis(np.linalg.norm,1,f_mock)\n",
    "frob_f_double = np.apply_along_axis(np.linalg.norm,1,f_double)\n",
    "\n",
    "\n",
    "## get std and mean distance from zero for each condition\n",
    "\n",
    "far_std = np.mean(far_pairwise_dists)\n",
    "#far_std = np.std(frob_f)\n",
    "far_mean = np.mean(frob_f)\n",
    "\n",
    "close_std = np.mean(close_pairwise_dists)\n",
    "#close_std = np.std(frob_f_double)\n",
    "close_mean = np.mean(frob_f_double)\n",
    "\n",
    "\n",
    "far_cv = far_std/far_mean\n",
    "close_cv = close_std/close_mean\n",
    "\n",
    "print close_std,  far_std\n",
    "print 'Far CV = {}, Close CV = {}'.format(far_cv, close_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print far_std, close_std\n",
    "print far_mean, close_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### characterize \"context difference vector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get difference between close and far\n",
    "to_inspect = 'category'\n",
    "categories = ['bird','car','chair','dog']\n",
    "this_category = categories[0]\n",
    "d = []\n",
    "for i, this_category in enumerate(categories):\n",
    "    c,f, obj_list = subset_dataframe_by_condition(F2,\n",
    "                                        to_inspect=to_inspect,\n",
    "                                        this_category=this_category) ## get subset of features\n",
    "\n",
    "    _d = c.sub(f)\n",
    "    if len(d)==0:\n",
    "        d = _d\n",
    "    else:\n",
    "        d = pd.concat((d,_d),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## visualize how well aligned the difference vectors are within a category\n",
    "dist_metric = 'correlation'\n",
    "D = 1 - pairwise_distances(d,metric=dist_metric)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.matshow(D, cmap=plt.cm.Spectral,vmin=-1.,vmax=1.)\n",
    "plt.colorbar(fraction=0.04)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "t = plt.xticks(range(len(ordered_objs)), ordered_objs, fontsize=10,rotation='vertical')\n",
    "t = plt.yticks(range(len(ordered_objs)), ordered_objs, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seems to be that for two of the categories, the difference vectors are all highly correlated with each other (bird, car), but this is less apparent for two of the other categories (chair, dog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measuring relative \"spikiness\" in close vs. far sketches?\n",
    "Perhaps using Frobenius norm (root sum squares of each element in the vector),\n",
    " which is minimized for uniform vector, and larger for spikier vectors (with larger values concentrated in fewer dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## helpers\n",
    "def entropy(probs):    \n",
    "    return - 1 * sum(map(lambda x: x * np.log(x),probs))\n",
    "\n",
    "def KL_div_uniform(probs):\n",
    "    unif_p = 1/len(probs)\n",
    "    return sum(map(lambda x: unif_p * np.log(unif_p/x),probs))\n",
    "\n",
    "def softmax(X):\n",
    "    '''\n",
    "    input: X is a (1 x N) array\n",
    "    output: 1 x N array\n",
    "    '''\n",
    "    return np.exp(X)/np.sum(np.exp(X))\n",
    "\n",
    "\n",
    "def minmaxscale(X):\n",
    "\n",
    "    return (X-np.min(X))/(np.max(X)-np.min(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## extract just the feature columns of the feature matrix, and break out by context \n",
    "to_inspect = 'all'\n",
    "c,f, obj_list = subset_dataframe_by_condition(F2,to_inspect=to_inspect)\n",
    "\n",
    "## convert to numpy array\n",
    "c = np.array(c)\n",
    "f = np.array(f)\n",
    "\n",
    "scale_mode='minmax'  ## minmax or softmax\n",
    "\n",
    "if scale_mode == 'softmax':\n",
    "    ## softmax\n",
    "    soft_c = np.apply_along_axis(softmax,1,c)\n",
    "    soft_f = np.apply_along_axis(softmax,1,f)\n",
    "    ## get \"spikiness\" index on close and far average sketches for each object\n",
    "    close_norm = np.apply_along_axis(np.linalg.norm,1,soft_c)\n",
    "    far_norm = np.apply_along_axis(np.linalg.norm,1,soft_f)\n",
    "    diff_norm = close_norm - far_norm  \n",
    "elif scale_mode == 'minmax':\n",
    "    ##minmax\n",
    "\n",
    "    minmax_c = np.apply_along_axis(minmaxscale,1,c)\n",
    "    minmax_f = np.apply_along_axis(minmaxscale,1,f)\n",
    "    ## get \"spikiness\" index on close and far average sketches for each object\n",
    "    close_norm = np.apply_along_axis(np.linalg.norm,1,minmax_c)\n",
    "    far_norm = np.apply_along_axis(np.linalg.norm,1,minmax_f)\n",
    "    diff_norm = close_norm - far_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = plt.hist(diff_norm)\n",
    "print 'Mean close spikiness = {}  (higher values are spikier)'.format(np.mean(close_norm).round(4))\n",
    "print 'Mean far spikiness = {}'.format(np.mean(far_norm).round(4))\n",
    "print 'Mean close-far difference on spikiness = {}'.format(np.mean(diff_norm).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp = sum(np.array(diff_norm)>0)\n",
    "print 'Number of objects for which close was spikier than far: {}'.format(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if scale_mode == 'softmax':\n",
    "    mean_soft_c= np.mean(soft_c,0)\n",
    "    mean_soft_f = np.mean(soft_f,0)\n",
    "    l = plt.plot(soft_c[1,:].T,label='close')\n",
    "    l = plt.plot(soft_f[1,:].T,label='far')\n",
    "    plt.legend()\n",
    "    plt.xlabel('feature dim')\n",
    "    plt.ylabel('softmax feature weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if scale_mode == 'minmax':\n",
    "    l = plt.plot(minmax_c[0,:].T,label='close')\n",
    "    l = plt.plot(minmax_f[0,:].T,label='far')\n",
    "    plt.legend()\n",
    "    plt.xlabel('feature dim')\n",
    "    plt.ylabel('minmax feature weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: Looks like the pattern of peaks might be preserved between close and far, just taller peaks for close, \n",
    "## but rank ordering of peaks is preserved? If that is the case, then we predict that the Spearman correlation \n",
    "## coefficient will be high between close and far vectors for each object... \n",
    "import scipy.stats\n",
    "run= False\n",
    "if run==True:\n",
    "    for i in range(32):\n",
    "        plt.figure()\n",
    "        s = plt.scatter(soft_c[i,:],soft_f[i,:])\n",
    "        plt.title(stats.spearmanr(soft_c[i,:],soft_f[i,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if scale_mode == 'minmax':\n",
    "    s = plt.scatter(minmax_c[0,:],minmax_f[0,:])\n",
    "    import scipy.stats\n",
    "    print stats.spearmanr(minmax_c[0,:],minmax_f[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zip(obj_list,diff_norm,np.arange(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman rank correlation matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_CI(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), stats.sem(a)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return  (m-h).round(3), (m+h).round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "close_objs,far_objs, obj_list = subset_dataframe_by_condition(F2, to_inspect= 'all' )\n",
    "print obj_list\n",
    "close_objs_sm = np.apply_along_axis(softmax,1,close_objs)\n",
    "far_objs_sm = np.apply_along_axis(softmax,1,far_objs)\n",
    "all_objs_sm= np.vstack((np.array(close_objs_sm),np.array(far_objs_sm)))\n",
    "corr_matrix, p = stats.spearmanr(all_objs_sm.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(9,9))\n",
    "plt.matshow(corr_matrix, cmap=plt.cm.Spectral,vmin=-1.,vmax=1.)\n",
    "plt.colorbar(fraction=0.05)\n",
    "plt.xlabel('close                                      far')\n",
    "plt.ylabel('far                                      close')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "close_far_labels =  [s +'_close' for s in ordered_objs]+[s +'_far' for s in ordered_objs]\n",
    "t = plt.xticks(range(len(ordered_objs)*2), close_far_labels, fontsize=10,rotation='vertical')\n",
    "t = plt.yticks(range(len(ordered_objs)*2), close_far_labels, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim=corr_matrix.shape[0]\n",
    "half_dim = int(corr_matrix.shape[0]/2)\n",
    "cf_corr = corr_matrix[half_dim:dim,:half_dim]\n",
    "#cf_corr = stats.zscore(cf_corr)\n",
    "\n",
    "plt.figure(figsize(8,8))\n",
    "plt.matshow(cf_corr, cmap=plt.cm.Spectral,vmin=-1.,vmax=1.)\n",
    "plt.colorbar(fraction=0.05)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "t = plt.xticks(range(len(ordered_objs)), [s +'_close' for s in ordered_objs], fontsize=10,rotation='vertical')\n",
    "t = plt.yticks(range(len(ordered_objs)), [s +'_far' for s in ordered_objs], fontsize=10)\n",
    "cf_matched_mean = np.mean(np.diag(cf_corr))\n",
    "triu_inds = np.triu_indices(half_dim,1)\n",
    "cf_off_diag = cf_corr[triu_inds]\n",
    "cf_permuted_mean = cf_off_diag.mean()\n",
    "print 'The mean of corr coefficients for close-far pairs of the same object is {},CI:{}\\\n",
    " while the mean of the corr coefficients for permuted pairs is {},CI:{}'.format(cf_matched_mean.round(3),\\\n",
    "                                                                                    calculate_CI(np.diag(cf_corr)),\\\n",
    "                                                                               cf_permuted_mean.round(3), calculate_CI(cf_off_diag))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "close_objs_sm_xcat = close_objs_sm\n",
    "far_objs_sm_xcat = far_objs_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_plots = False\n",
    "\n",
    "for i, this_category in enumerate(categories):\n",
    "\n",
    "    close_objs,far_objs, obj_list = subset_dataframe_by_condition(F2, to_inspect= 'category',this_category=this_category )\n",
    "    close_objs_sm = np.apply_along_axis(softmax,1,close_objs)\n",
    "    far_objs_sm = np.apply_along_axis(softmax,1,far_objs)\n",
    "    all_objs_sm= np.vstack((np.array(close_objs_sm),np.array(far_objs_sm)))\n",
    "#     corr_matrix, p = stats.spearmanr(all_objs_sm.T)\n",
    "    corr_matrix = np.corrcoef(all_objs_sm)\n",
    "    cf_corr = corr_matrix[half_dim:dim,:half_dim]\n",
    "    \n",
    "    if render_plots:\n",
    "        plt.figure(figsize(8,8))\n",
    "        plt.matshow(corr_matrix, cmap=plt.cm.Spectral,vmin=-1.,vmax=1.)\n",
    "        plt.colorbar(fraction=0.05)\n",
    "        plt.title(this_category)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlabel('close                     far')\n",
    "        plt.ylabel('far                     close')\n",
    "        close_far_labels =  [s +'_close' for s in ordered_objs]+[s +'_far' for s in ordered_objs]\n",
    "        dim=corr_matrix.shape[0]\n",
    "        half_dim = int(corr_matrix.shape[0]/2)\n",
    "        plt.figure(figsize(8,8))\n",
    "        plt.matshow(cf_corr, cmap=plt.cm.Spectral,vmin=-1.,vmax=1.)\n",
    "        plt.colorbar(fraction=0.04)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title('{} close-far'.format(this_category))\n",
    "    #t = plt.xticks(range(len(ordered_objs)), [s +'_close' for s in ordered_objs], fontsize=10,rotation='vertical')\n",
    "    #t = plt.yticks(range(len(ordered_objs)), [s +'_far' for s in ordered_objs], fontsize=10)\n",
    "    cf_matched_mean = np.mean(np.diag(cf_corr))\n",
    "    triu_inds = np.triu_indices(half_dim,1)\n",
    "    cf_off_diag = cf_corr[triu_inds]\n",
    "    cf_permuted_mean = cf_off_diag.mean()\n",
    "    print 'Category: {} | Mean of corrs for close-far pairs of the same object is {},CI:{}\\\n",
    "     while the mean of the corr coeffs for permuted pairs is {},CI:{}'.format(this_category, cf_matched_mean.round(3),\\\n",
    "                                                                                    calculate_CI(np.diag(cf_corr)),\\\n",
    "                                                                               cf_permuted_mean.round(3), calculate_CI(cf_off_diag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(close_objs_sm[2,:],close_objs_sm[1,:])\n",
    "plt.xlabel('a close obj')\n",
    "plt.ylabel('another close obj in the same category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sketch level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## commenting this out b/c this sorting happens inside the subset_dataframe function\n",
    "## so this won't affect the result\n",
    "# a= F.sort_values(['category', 'target','condition'], ascending=[True, True,True]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ac,af, obj_list = subset_dataframe_by_condition(F0,to_inspect='all' )\n",
    "ac_sm = np.apply_along_axis(softmax,1,ac)\n",
    "af_sm = np.apply_along_axis(softmax,1,af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objmat= np.vstack((ac_sm,af_sm))\n",
    "corr_matrix, p = stats.spearmanr(objmat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm, pval= stats.spearmanr(af.T)\n",
    "cm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(8,8))\n",
    "plt.matshow(corr_matrix, cmap=plt.cm.Spectral,vmin=-1.,vmax=1.)\n",
    "plt.colorbar(fraction=0.05)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('close                      far')\n",
    "plt.ylabel('far                      close')\n",
    "plt.title('object distances x condition')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "plt.figure(figsize(8,8))\n",
    "plt.matshow(corr_matrix, cmap=plt.cm.Spectral,vmin=-1.,vmax=1.)\n",
    "plt.colorbar(fraction=0.05)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('close                      far')\n",
    "plt.ylabel('far                      close')\n",
    "plt.title('object distances x condition')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### relationship between numstrokes and arclength features in unaggregated feature matrix (at sketch level)\n",
    "I.e., Check how redundant the num strokes vector is with the arclength vector across sketches (how correlated are the 23 vectors across sketches within an object?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "if dataset=='normalized':\n",
    "    assert len(np.unique(F[arclength_cols].sum(axis=1).round(10)))==1\n",
    "    assert len(np.unique(F[numstrokes_cols].sum(axis=1).round(10)))==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## inspect particular category\n",
    "this_category = 'chair'\n",
    "category_inds = F['category']==this_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## extract matrix form of just the arclength and numstrokes columns\n",
    "to_inspect = 'all'\n",
    "if to_inspect == 'category':\n",
    "    arcF = np.array(F[category_inds][arclength_cols])\n",
    "    numF = np.array(F[category_inds][numstrokes_cols])\n",
    "elif to_inspect == 'all':\n",
    "    arcF = np.array(F[arclength_cols])\n",
    "    numF = np.array(F[numstrokes_cols])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a sense for how correspondent the numstrokes and arclength features are (how redundant are they?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## get distances between sketches using either numstrokes (numF) or arclength (arcF)\n",
    "feat_type = numF\n",
    "D = pairwise_distances(feat_type,metric='euclidean')\n",
    "dists_within_metric = D[np.triu_indices(D.shape[0], k=1)]\n",
    "print 'Mean euclidean distance between sketches using a single metric (numstrokes or arclength): {}'.format(np.mean(dists_within_metric).round(5))\n",
    "\n",
    "## get correspondence between numstrokes and arclength feature\n",
    "dists_btw_metrics = [distance.euclidean(arcV,numV) for(arcV, numV) in zip(arcF,numF)]\n",
    "print 'Mean euclidean distance within sketches between arclength and numstrokes feature vectors: {}'.format(np.mean(dists_btw_metrics).round(5))\n",
    "\n",
    "## get distance between randomly sampled numstroke and arclength vector from different sketches\n",
    "nIter = 1000\n",
    "num_sketches = np.shape(arcF)[0]\n",
    "dists_btw_metrics_permuted = []\n",
    "for this_iter in np.arange(nIter):\n",
    "    rand_inds = np.random.RandomState(this_iter).choice(num_sketches,2)\n",
    "    arcV = arcF[rand_inds[0],:]\n",
    "    numV = numF[rand_inds[1],:]\n",
    "    dists_btw_metrics_permuted.append(distance.euclidean(arcV,numV))\n",
    "print \"Mean euclidean distance between randomly sampled sketches' arclength and numstrokes feature vectors: {}\".format(np.mean(dists_btw_metrics_permuted).round(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hmm, maybe a better way to do this. How about learning linear mapping from numstrokes to arclength and computing variance explained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Assume linear function to map from X to y\n",
    "## y = Xb\n",
    "X = arcF \n",
    "y = numF\n",
    "\n",
    "## solve for b directly using pseudo inverse\n",
    "b = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "yhat = X.dot(b)\n",
    "\n",
    "## get proportion of variance explained\n",
    "## https://en.wikipedia.org/wiki/Fraction_of_variance_unexplained\n",
    "## Fraction of variance unexplained, FVU = SS_err / SS_tot\n",
    "SS_err = np.sum([distance.euclidean(_yhat,_y)**2 for (_yhat, _y) in zip(yhat,y)])\n",
    "SS_tot = np.sum([distance.euclidean(_y,y.mean(0))**2 for _y in y])\n",
    "FVU = SS_err / SS_tot\n",
    "print 'Fraction of variance unexplained after learning linear mapping to arclength to get numstrokes is: {}.'.format(FVU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
