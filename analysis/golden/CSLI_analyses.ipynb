{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries and setting up directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import urllib, cStringIO\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "import sys\n",
    "\n",
    "from svgpathtools import parse_path\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('../..')\n",
    "analysis_dir = os.getcwd()\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "features_dir= os.path.join(results_dir,'features')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'experiments'))\n",
    "sketch_dir = os.path.abspath(os.path.join(proj_dir,'sketches'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'analysis'))\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)  \n",
    "\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)  \n",
    "\n",
    "if not os.path.exists(features_dir):\n",
    "    os.makedirs(features_dir)\n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'analysis'))        \n",
    "    \n",
    "# Assign variables within imported analysis helpers\n",
    "import analysis_helpers as h\n",
    "if sys.version_info[0]>=3:\n",
    "    from importlib import reload\n",
    "reload(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_df(X):\n",
    "    if 'Unnamed: 0' in X.columns:\n",
    "        X = X.drop(columns=['Unnamed: 0'])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read in raw dataframe with subsetted data for 3 annotations per sketch\n",
    "\n",
    "D=cleanup_df(pd.read_csv(os.path.join(csv_dir,'raw_df.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating spline and stroke level dataframes for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the list of unique labels applied to sketches\n",
    "unique_labels = np.unique(D.label.values)\n",
    "\n",
    "## Removing Nones and obviously wrong super long lables\n",
    "unique_labels = [i for i in unique_labels if i is not None]\n",
    "unique_labels = [i for i in unique_labels if len(i)<900]\n",
    "\n",
    "print 'we have {} unique labels'.format(len( unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Create empty dictionary with categories as keys. We will use this to store part occurrence data for our categories\n",
    "label_vect_dict = {unique_cats[0]:None,unique_cats[1]:None,unique_cats[2]:None,unique_cats[3]:None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create vectors that contain the number of part instances in each sketch\n",
    "\n",
    "for category in unique_cats:\n",
    "    DS= D[D['category']==category]\n",
    "    unique_sketches_in_cat = np.unique(DS['sketch_id'])\n",
    "    unique_labels_in_cat = np.unique(DS['label'])\n",
    "    ## initialize matrix that has the correct dimensions\n",
    "    Label_Vec = np.zeros((len(unique_sketches_in_cat),len(unique_labels_in_cat)), dtype=int)\n",
    "    unique_labels_in_cat= np.array(unique_labels_in_cat)\n",
    "    for s,this_sketch in enumerate(unique_sketches_in_cat):\n",
    "        label_vec = np.zeros(len(unique_labels_in_cat),dtype=int)\n",
    "        DSS = DS[DS['sketch_id']==this_sketch]\n",
    "        annotation_ids = np.unique(DSS['annotation_id'].values)    \n",
    "        for this_annotation in annotation_ids:\n",
    "            DSA = DSS[DSS['annotation_id']==this_annotation]\n",
    "            label_list = DSA.label.values\n",
    "            for this_label in label_list:\n",
    "                label_ind = unique_labels_in_cat==this_label\n",
    "                label_vec[label_ind] += 1\n",
    "            \n",
    "        Label_Vec[s,:]=label_vec/num_annots\n",
    "    label_vect_dict[category]= Label_Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels=[]\n",
    "valid_labels_dict={}\n",
    "for category in unique_cats:\n",
    "    vect = label_vect_dict[category]\n",
    "    thresh = 50\n",
    "    #print 'These are the labels that appear at least {} times:'.format(thresh)\n",
    "    #print unique_labels[np.sum(Label_Vec,0)>thresh]\n",
    "    unique_labels_in_cat = np.unique(D[D['category']==category]['label'])\n",
    "    plot_labels= unique_labels_in_cat[np.sum(vect,0)>thresh]\n",
    "    valid_labels_dict[category]=plot_labels\n",
    "    valid_labels.append(plot_labels)\n",
    "\n",
    "\n",
    "    prop_labels=[]\n",
    "    for part in plot_labels:\n",
    "        DS=D[D['category']==category]\n",
    "        prop_labels.append(DS[DS['label']==part]['annotation_id'].nunique()/DS['annotation_id'].nunique())\n",
    "    \n",
    "    \n",
    "    sns.set_context('talk')\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.ylim(0,1)\n",
    "    h = plt.bar(plot_labels,prop_labels)\n",
    "    plt.title('Proportion of {} annotations with labels'.format(category))\n",
    "    plt.ylabel('proportion of annotations')\n",
    "    plt.xlabel('Part')\n",
    "    \n",
    "##flattening valid labels\n",
    "valid_labels = [item for sublist in valid_labels for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a spline-level df where the modal label is set as the 'true' label for any given spline\n",
    "spline_df= D.groupby('spline_id').agg(lambda x: Counter(x).most_common(1)[0][0])\n",
    "spline_df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating a stroke-level dataframe that takes the mode value of annotation for its children splines to set as its\n",
    "##label value\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "stroke_svgs=OrderedDict()\n",
    "for category in unique_cats:\n",
    "    DS=D[D['category']==category]\n",
    "    for sketch in np.unique(DS['sketch_id']):\n",
    "        DSS=DS[DS['sketch_id']==sketch]\n",
    "        for stroke in np.unique(DSS['stroke_num']):\n",
    "            DSA=DSS[DSS['stroke_num']==stroke]\n",
    "            DSA=DSA.reset_index()\n",
    "            stroke_svgs[DSA['stroke_id'][0]] = DSA['sketch_svg_string'][0][stroke]\n",
    "\n",
    "            \n",
    "            \n",
    "stroke_svg_df= pd.DataFrame.from_dict(stroke_svgs, orient='index')    \n",
    "stroke_group_data= D.groupby('stroke_id').agg(lambda x: Counter(x).most_common(1)[0][0])\n",
    "labels= pd.DataFrame(stroke_group_data[['sketch_id','label','stroke_num','condition','target','category','outcome']])\n",
    "stroke_df=pd.merge(stroke_svg_df,labels,left_index=True, right_index =True)\n",
    "stroke_df.reset_index(level=0, inplace=True)\n",
    "stroke_df=stroke_df.rename(index=str, columns={\"index\": \"stroke_id\", 0: \"svg\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding total arclength information to stroke dataframe\n",
    "from svgpathtools import parse_path\n",
    "import svgpathtools\n",
    "\n",
    "def calculate_arclength(svg):\n",
    "    try:\n",
    "        arclength= parse_path(svg).length()\n",
    "    except ZeroDivisionError:\n",
    "        print 'zero div error'\n",
    "        arclength = 0\n",
    "    return arclength\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df['arc_length'] = stroke_df['svg'].apply(calculate_arclength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating feature vectors and normalizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###This is where we make a num unique labels * 2 X number of sketches vector \n",
    "\n",
    "feature_vec = np.zeros((len(stroke_df.sketch_id.unique()),len(valid_labels)*2), dtype=int)\n",
    "ind=0\n",
    "start_pos=0\n",
    "end_pos=0\n",
    "meta_list=[]\n",
    "cols = ['sketch_id','target','condition','category','outcome']\n",
    "\n",
    "for cat in unique_cats:\n",
    "  \n",
    "    DS= stroke_df[stroke_df['category']==cat]\n",
    "    unique_labels_in_cat=valid_labels_dict[cat]\n",
    "    unique_sketches_in_cat=DS['sketch_id'].unique()\n",
    "    start_pos = end_pos\n",
    "    end_pos+= len(unique_labels_in_cat)\n",
    "    print start_pos, end_pos\n",
    "    Label_Vec = np.zeros((len(unique_sketches_in_cat),len(unique_labels_in_cat)*2), dtype=int)\n",
    "    arc_length_vec = np.zeros((len(unique_sketches_in_cat),len(valid_labels_dict[cat])), dtype=int)\n",
    "    for s,sketch in enumerate(unique_sketches_in_cat):\n",
    "        \n",
    "        label_vec = np.zeros(len(unique_labels_in_cat),dtype=int)\n",
    "        arc_vec = np.zeros(len(unique_labels_in_cat),dtype=int)\n",
    "        DSA=DS[DS['sketch_id']==sketch]\n",
    "      \n",
    "        meta_list.append(pd.Series([DSA['sketch_id'],DSA['target'].unique(),DSA['condition'].unique(),DSA['category'].unique(),DSA['outcome'].unique()], index=cols))\n",
    "        label_list = DSA.label.values        \n",
    "        for label in label_list:\n",
    "            if label in unique_labels_in_cat:\n",
    "                label_ind = unique_labels_in_cat==label\n",
    "                label_vec[label_ind] += 1\n",
    "        for label in unique_labels_in_cat:\n",
    "            DSB=DSA[DSA['label']==label]\n",
    "            label_ind = unique_labels_in_cat==label\n",
    "            arc_vec[label_ind] = DSB['arc_length'].sum()\n",
    "            \n",
    "        \n",
    "        feature_vec[ind,start_pos:end_pos]=label_vec\n",
    "        feature_vec[ind,start_pos+len(valid_labels):end_pos+len(valid_labels)]=arc_vec\n",
    "        ind+=1\n",
    "feature_vec_meta = pd.DataFrame(meta_list, columns=cols)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Changing column values from np arrays to strings/boolean\n",
    "\n",
    "def arr_to_str(arr):\n",
    "    return (arr[0])\n",
    "feature_vec_meta['target']=feature_vec_meta['target'].apply(arr_to_str)\n",
    "feature_vec_meta['condition']=feature_vec_meta['condition'].apply(arr_to_str)\n",
    "feature_vec_meta['category']=feature_vec_meta['category'].apply(arr_to_str)\n",
    "feature_vec_meta['outcome']=feature_vec_meta['outcome'].apply(arr_to_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df= pd.DataFrame(feature_vec, columns=[s + '_numstrokes' for s in valid_labels]+[s + '_total_arclength' for s in valid_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating a compressed version of the feature df with no duplicates for parts\n",
    "\n",
    "labs_numstrokes=[]\n",
    "labs_total_arclength=[]\n",
    "for lab in np.unique(valid_labels):\n",
    "    labs_numstrokes.append(lab +'_numstrokes')\n",
    "    labs_total_arclength.append(lab+'_total_arclength')\n",
    "feature_df_labs=labs_numstrokes+labs_total_arclength   \n",
    "feature_df_final= pd.DataFrame(columns=feature_df_labs)\n",
    "\n",
    "\n",
    "for this_lab in feature_df_labs:\n",
    "    duplicates=[col for col in feature_df if col.startswith(this_lab)]\n",
    "    feature_df_final[this_lab]= feature_df[duplicates].sum(axis=1)\n",
    "feature_df = feature_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check to make sure the df looks okay\n",
    "assert len(feature_df.columns)==len(np.unique(feature_df.columns))\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing within row within measure (numstrokes/arclength) \n",
    "\n",
    "feature_df.iloc[:,0:int(len(feature_df.columns)/2)]=feature_df.iloc[:,0:int(len(feature_df.columns)/2)].div(feature_df.iloc[:,0:int(len(feature_df.columns)/2)].sum(axis=1),axis=0)\n",
    "\n",
    "feature_df.iloc[:,int(len(feature_df.columns)/2):int(len(feature_df.columns))]=feature_df.iloc[:,int(len(feature_df.columns)/2):int(len(feature_df.columns))].div(feature_df.iloc[:,int(len(feature_df.columns)/2):int(len(feature_df.columns))].sum(axis=1),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Execute this if we want to save a non-zscore matrix\n",
    "run=True\n",
    "if run==True:\n",
    "    feature_df.to_csv(os.path.join(features_dir,'semantic_parts_sketch_features_compressed_non-whitened.csv'))\n",
    "run=False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-scoring within columns\n",
    "\n",
    "columns=list(feature_df.columns)\n",
    "for this_col in columns:\n",
    "    feature_df[this_col]=(feature_df[this_col] - feature_df[this_col].mean())/feature_df[this_col].std(ddof=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving out files as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_csv(os.path.join(features_dir,'semantic_parts_sketch_features_compressed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(features_dir, 'semantic_parts_sketch_features'),feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vec_meta.to_csv(os.path.join(features_dir,'semantic_parts_sketch_meta.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results : \"Summer Analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-annotator reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the number of unique labels assigned to a given spline across annotations\n",
    "num_diff_annots = []\n",
    "for this_cat in unique_cats:\n",
    "    DS=D[D['category']==this_cat]\n",
    "    labels = valid_labels_dict[this_cat]\n",
    "    unique_sketches_in_cat=np.unique(DS['sketch_id'])\n",
    "    \n",
    "\n",
    "   \n",
    "    for this_sketch_id in unique_sketches_in_cat:\n",
    "        DSA=DS[DS['sketch_id']==this_sketch_id]\n",
    "        unique_splines = np.unique(DSA['cumulative_spline_num'])\n",
    "        for i,this_spline in enumerate(unique_splines):\n",
    "            DSB =DSA[DSA['cumulative_spline_num']==this_spline]\n",
    "            numannots= 4-len(np.unique(DSB['label']))\n",
    "            if numannots==0:\n",
    "                numannots=1\n",
    "            num_diff_annots.append(numannots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting variability in spline annots\n",
    "h= plt.hist(num_diff_annots, bins= range(1,5), align='left', density='True')\n",
    "plt.title('Inter-annotator reliability')\n",
    "plt.ylabel('proportion of splines')\n",
    "plt.xlabel('Annotator agreement on label')\n",
    "plt.xticks([1,2,3],['0/3','2/3','3/3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stroke-part relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_cat in unique_cats:\n",
    "    labels = valid_labels_dict[this_cat]\n",
    "    DS=spline_df[spline_df['category']==this_cat]\n",
    "    spline_annots_per_stroke = []\n",
    "    unique_sketches_in_cat= np.unique(DS['sketch_id'])\n",
    "    for this_sketch_id in unique_sketches_in_cat:\n",
    "        DSA=DS[DS['sketch_id']==this_sketch_id]\n",
    "        unique_strokes = np.unique(DSA['stroke_num'])\n",
    "        for i,this_stroke in enumerate(unique_strokes):\n",
    "            DSB =DSA[DSA['stroke_num']==this_stroke]\n",
    "            numlabels= DSB['label'].nunique()\n",
    "            spline_annots_per_stroke.append(numlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h= plt.hist(spline_annots_per_stroke, bins =range(1,8), align='left', density=\"True\")\n",
    "plt.title('Within-stroke label agreement')\n",
    "plt.ylabel('proportion of strokes')\n",
    "plt.xlabel('number of different labels within stroke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_cat in unique_cats:\n",
    "    DS=stroke_df[stroke_df['category']==this_cat]\n",
    "    labels= valid_labels_dict[this_cat]\n",
    "    strokes_in_part_vect = np.zeros((len(np.unique(DS['sketch_id']))*len(labels),3), dtype='|a1000')\n",
    "    ind=0\n",
    "    for this_sketch in np.unique(DS['sketch_id']):    \n",
    "        DSA= DS[DS['sketch_id']==this_sketch]\n",
    "        for this_label in labels:\n",
    "            DSB=DSA[DSA['label']==this_label]\n",
    "            strokes_in_part_vect[ind,]=[this_sketch, this_label,len(np.unique(DSB['stroke_num']))]\n",
    "            ind+=1\n",
    "    strokes_in_part_vect=strokes_in_part_vect[~np.all(strokes_in_part_vect == '', axis=1)]\n",
    "    strokes_in_part_df= pd.DataFrame(strokes_in_part_vect, columns=['sketch_id','part','num_strokes'])\n",
    "    strokes_in_part_df['num_strokes']=pd.to_numeric(strokes_in_part_df['num_strokes'])\n",
    "    plt.figure()\n",
    "    b=sns.barplot(x='part',y='num_strokes',data=strokes_in_part_df,ci=95,capsize=0.3, errwidth= 3)\n",
    "    for item in b.get_xticklabels():\n",
    "        item.set_rotation(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-streak analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating a dictionary of sketch_id with associated part sequences\n",
    "seq_dict={}\n",
    "for this_sketch in np.unique(stroke_df['sketch_id']):\n",
    "    parts_list=[]\n",
    "    DS=stroke_df[stroke_df['sketch_id']==this_sketch]\n",
    "    for i, row in DS.iterrows():\n",
    "        parts_list.append(stroke_df['label'][i])\n",
    "    seq_dict[this_sketch]=parts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##functions for getting 'mean streak_length' from a particular sketch for ground truth and scrambled part orders\n",
    "\n",
    "import random\n",
    "\n",
    "def get_mean_streak(sketch_id):\n",
    "    parts = seq_dict[sketch_id]\n",
    "    streak_counter=1\n",
    "    list_of_streaks=[]\n",
    "    for obj in range(len(parts)-1):\n",
    "        if parts[obj]==parts[obj+1]:\n",
    "            streak_counter+=1\n",
    "        else:\n",
    "            list_of_streaks.append(streak_counter)\n",
    "            streak_counter=1 \n",
    "    list_of_streaks.append(streak_counter)\n",
    "    return np.mean(list_of_streaks)\n",
    "\n",
    "def get_scramble_mean_streak(sketch_id):\n",
    "    parts = seq_dict[sketch_id]\n",
    "    scram_parts=random.sample(parts,len(parts))\n",
    "    streak_counter=1\n",
    "    list_of_streaks=[]\n",
    "    for obj in range(len(scram_parts)-1):\n",
    "        if scram_parts[obj]==scram_parts[obj+1]:\n",
    "            streak_counter+=1\n",
    "        else:\n",
    "            list_of_streaks.append(streak_counter)\n",
    "            streak_counter=1 \n",
    "    list_of_streaks.append(streak_counter)\n",
    "    return np.mean(list_of_streaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating over all sketches to get mean streakiness for each sketch_id\n",
    "\n",
    "gt_streak_mean={}\n",
    "for this_cat in unique_cats:\n",
    "    DS= stroke_df[stroke_df['category']==this_cat]\n",
    "    streak_mean_list=[]\n",
    "    for this_sketch in np.unique(DS['sketch_id']):\n",
    "        streak_mean_list.append(get_mean_streak(this_sketch))\n",
    "    gt_streak_mean[this_cat]=np.mean(streak_mean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streak_diff_dict={}\n",
    "for this_cat in unique_cats:\n",
    "    mean_streak_diff_list=[]\n",
    "    DS=stroke_df[stroke_df['category']==this_cat]\n",
    "    for i in range(1000):\n",
    "        scrambled_streaks=[] \n",
    "        real_streaks=[]\n",
    "        for sketch in np.unique(DS['sketch_id']):\n",
    "            scrambled_streaks.append(get_scramble_mean_streak(sketch))\n",
    "            real_streaks.append(get_mean_streak(sketch))\n",
    "        mean_streak_diff_list.append(np.mean(real_streaks)-np.mean(scrambled_streaks))\n",
    "    streak_diff_dict[this_cat]=mean_streak_diff_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CIPlot(category): \n",
    "    stroke_df_lite_ss=stroke_df[stroke_df['category']==category]\n",
    "    mean_streak_diff_list=[]\n",
    "    for i in range(1000):\n",
    "        this_round_scrambled_streak=[] \n",
    "        this_round_real_streak=[]\n",
    "        for sketch in np.unique(stroke_df_lite_ss['sketch_id']):\n",
    "            this_round_real_streak.append(get_mean_streak(sketch))\n",
    "            this_round_scrambled_streak.append(get_scramble_mean_streak(sketch))\n",
    "        mean_streak_diff_list.append(np.mean(this_round_real_streak)-np.mean(this_round_scrambled_streak))\n",
    "    perm_observed_mean_streak_diff = np.mean(mean_streak_diff_list)    \n",
    "    lb=np.percentile(mean_streak_diff_list,2.5)\n",
    "    ub=np.percentile(mean_streak_diff_list,97.5)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    h=sns.distplot(mean_streak_diff_list,kde=False,hist=True,norm_hist=False)\n",
    "    plt.axvline(perm_observed_mean_streak_diff, color='yellow', linestyle='solid', linewidth=2)\n",
    "    plt.axvline(lb, color='orange', linestyle='solid', linewidth=2)\n",
    "    plt.axvline(ub, color='orange', linestyle='solid', linewidth=2)\n",
    "    plt.title(category)\n",
    "    plt.ylabel('count')\n",
    "    plt.xlabel('streak length difference')\n",
    "    plt.legend(['mean','95% CI'], ncol=2, bbox_to_anchor=(1, 1.05))\n",
    "    \n",
    "    plt.savefig(os.path.join(plot_dir,'Streakiness Diff'),edgecolor='w',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return np.mean(mean_streak_diff_list), np.std(mean_streak_diff_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CIPlotCatCond(category,condition): \n",
    "    stroke_df_lite_ss=stroke_df[(stroke_df['category']==category)&(stroke_df['condition']==condition)]\n",
    "    mean_streak_diff_list=[]\n",
    "    for i in range(1000):\n",
    "        this_round_scrambled_streak=[] \n",
    "        this_round_real_streak=[]\n",
    "        for sketch in np.unique(stroke_df_lite_ss['sketch_id']):\n",
    "            this_round_real_streak.append(get_mean_streak(sketch))\n",
    "            this_round_scrambled_streak.append(get_scramble_mean_streak(sketch))\n",
    "        mean_streak_diff_list.append(np.mean(this_round_real_streak)-np.mean(this_round_scrambled_streak))\n",
    "    perm_observed_mean_streak_diff = np.mean(mean_streak_diff_list)    \n",
    "    lb=np.percentile(mean_streak_diff_list,2.5)\n",
    "    ub=np.percentile(mean_streak_diff_list,97.5)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    h=sns.distplot(mean_streak_diff_list,kde=False,hist=True,norm_hist=False)\n",
    "    plt.axvline(perm_observed_mean_streak_diff, color='yellow', linestyle='solid', linewidth=2)\n",
    "    plt.axvline(lb, color='orange', linestyle='solid', linewidth=2)\n",
    "    plt.axvline(ub, color='orange', linestyle='solid', linewidth=2)\n",
    "    plt.title('{}_{}'.format(category,condition))\n",
    "    plt.ylabel('count')\n",
    "    plt.xlabel('streak length difference')\n",
    "    plt.legend(['mean','95% CI'], ncol=2, bbox_to_anchor=(1, 1.05))\n",
    "    \n",
    "    plt.savefig(os.path.join(plot_dir,'mean_streak_difference_{}_{}'.format(category, condition)),edgecolor='w',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return perm_observed_mean_streak_diff, lb, ub\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_cat in unique_cats:\n",
    "    CIPlot(this_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_condition in np.unique(stroke_df['condition']):\n",
    "    for this_category in np.unique(stroke_df['category']):\n",
    "        CIPlotCatCond(this_category, this_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_cat in unique_cats:\n",
    "    plot_data= CIPlot(this_cat)\n",
    "    plt.figure\n",
    "    CI_data= np.array(plot_data[0]-plot_data[0]-2*plot_data[1], plot_data[0]-plot_data[0]+2*plot_data[1])\n",
    "    h= plt.bar([0,1,2],[0,plot_data[0],0],yerr=[0,CI_data,0],capsize=15)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Mean streakiness difference')\n",
    "    plt.xticks([0,1,2],['','',''])\n",
    "    plt.savefig(os.path.join(plot_dir,'{}_streak_diff'.format(this_cat)),edgecolor='w',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results : Sketch Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
