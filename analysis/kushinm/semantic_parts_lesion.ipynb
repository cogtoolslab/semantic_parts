{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import urllib, cStringIO\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from matplotlib import pylab, mlab, pyplot, colors\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from svgpathtools import parse_path, concatpaths\n",
    "import svgpathtools\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category = ConvergenceWarning )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import svg_rendering_helpers as srh\n",
    "if sys.version_info[0]>=3:\n",
    "    from importlib import reload\n",
    "reload(srh)\n",
    "\n",
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('../..')\n",
    "analysis_dir = os.getcwd()\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "features_dir= os.path.join(results_dir,'features')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'experiments'))\n",
    "sketch_dir = os.path.abspath(os.path.join(proj_dir,'sketches'))\n",
    "vgg_dir ='/Users/kushin/Documents/Github/semantic_parts/features'\n",
    "\n",
    "##create a dir for testing chair sketches for lesion\n",
    "\n",
    "chairs_dir = os.path.join(sketch_dir, 'chairs_only')\n",
    "\n",
    "test_dir = os.path.join(chairs_dir, 'test')\n",
    "control_dir = os.path.join(chairs_dir, 'control')\n",
    "intact_dir = os.path.join(chairs_dir, 'intact')\n",
    "lesion_dir = os.path.join(chairs_dir,'lesioned')\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'analysis'))\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)  \n",
    "\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)  \n",
    "\n",
    "if not os.path.exists(features_dir):\n",
    "    os.makedirs(features_dir)\n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'analysis'))        \n",
    "    \n",
    "# Assign variables within imported analysis helpers\n",
    "import analysis_helpers as h\n",
    "if sys.version_info[0]>=3:\n",
    "    from importlib import reload\n",
    "reload(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper dictionaries \n",
    "OBJECT_TO_CATEGORY = {\n",
    "    'basset': 'dog', 'beetle': 'car', 'bloodhound': 'dog', 'bluejay': 'bird',\n",
    "    'bluesedan': 'car', 'bluesport': 'car', 'brown': 'car', 'bullmastiff': 'dog',\n",
    "    'chihuahua': 'dog', 'crow': 'bird', 'cuckoo': 'bird', 'doberman': 'dog',\n",
    "    'goldenretriever': 'dog', 'hatchback': 'car', 'inlay': 'chair', 'knob': 'chair',\n",
    "    'leather': 'chair', 'nightingale': 'bird', 'pigeon': 'bird', 'pug': 'dog',\n",
    "    'redantique': 'car', 'redsport': 'car', 'robin': 'bird', 'sling': 'chair',\n",
    "    'sparrow': 'bird', 'squat': 'chair', 'straight': 'chair', 'tomtit': 'bird',\n",
    "    'waiting': 'chair', 'weimaraner': 'dog', 'white': 'car', 'woven': 'chair',\n",
    "}\n",
    "CATEGORY_TO_OBJECT = {\n",
    "    'dog': ['basset', 'bloodhound', 'bullmastiff', 'chihuahua', 'doberman', 'goldenretriever', 'pug', 'weimaraner'],\n",
    "    'car': ['beetle', 'bluesedan', 'bluesport', 'brown', 'hatchback', 'redantique', 'redsport', 'white'],\n",
    "    'bird': ['bluejay', 'crow', 'cuckoo', 'nightingale', 'pigeon', 'robin', 'sparrow', 'tomtit'],\n",
    "    'chair': ['inlay', 'knob', 'leather', 'sling', 'squat', 'straight', 'waiting', 'woven'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Helpers\n",
    "\n",
    "\n",
    "\n",
    "def subset_dataframe_by_condition(F,to_inspect='all',this_category='bird',this_object='bluejay'):\n",
    "    '''\n",
    "    input: F: dataframe (num_sketches x num_features)\n",
    "           to_inspect: a string indicating whether to subset by ['object','category','all']\n",
    "           this_category: IF to_inspect == 'category', then we define this to subset by that category only\n",
    "           this_object: IF to_inspect == 'object', then we define this to subset by that object only\n",
    "           \n",
    "    returns: two feature matrices, c and f, corresponding to the close and far subsetted feature matrices\n",
    "           \n",
    "    '''\n",
    "        \n",
    "    F = F.sort_values(by=['category','target'])\n",
    "\n",
    "    ## get context condition inds for subsetting dataframe\n",
    "    close_inds = F['condition'] == 'closer'\n",
    "    far_inds = F['condition'] == 'further'\n",
    "\n",
    "    ## if we want to inspect particular category\n",
    "    category_inds = F['category']==this_category\n",
    "\n",
    "    ## if we want to inspect particular object\n",
    "    obj_list = np.unique(F.target.values)\n",
    "    obj_inds = F['target']==this_object  \n",
    "    \n",
    "    ## get names of columns that contain stroke-count & arclength information\n",
    "    numstrokes_cols = [i for i in F.columns if i.split('_')[-1]=='numstrokes']\n",
    "    arclength_cols = [i for i in F.columns if i.split('_')[-1]=='arclength']\n",
    "    feat_cols = numstrokes_cols + arclength_cols\n",
    "    \n",
    "    if to_inspect == 'object':    \n",
    "        ## extract particular row corresponding to this OBJECT in each condition\n",
    "        f = F[(far_inds) & obj_inds][feat_cols].reset_index(drop=True)\n",
    "        c = F[(close_inds) & obj_inds][feat_cols].reset_index(drop=True)\n",
    "        obj_listf = F[(far_inds) & obj_inds]['target'].values\n",
    "        obj_listc = F[(close_inds) & obj_inds]['target'].values\n",
    "    elif to_inspect == 'category':\n",
    "        ## extract particular rows corresponding to this CATEGORY in each condition\n",
    "        f = F[(category_inds) & (far_inds)][feat_cols].reset_index(drop=True)\n",
    "        c = F[(category_inds) & (close_inds)][feat_cols].reset_index(drop=True)\n",
    "        obj_listf = F[(category_inds) & (far_inds)]['target'].values\n",
    "        obj_listc = F[(category_inds) & (close_inds)]['target'].values\n",
    "    elif to_inspect == 'all':\n",
    "        ## extract particular rows corresponding to each condition\n",
    "        f = F[far_inds][feat_cols].reset_index(drop=True)\n",
    "        c = F[close_inds][feat_cols].reset_index(drop=True) \n",
    "        obj_listf = F[far_inds]['target'].values\n",
    "        obj_listc = F[close_inds]['target'].values\n",
    "        \n",
    "    return c, f, obj_listc, obj_listf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in annotated sketch dataset| subsetted for sketches with 3 annotations\n",
    "D = pd.read_pickle(os.path.join(csv_dir, 'semantic_parts_annotated_data_pckl'))\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png= pd.read_csv(os.path.join(csv_dir, 'semantic_parts_annotated_pngstring.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the list of unique labels applied to sketches\n",
    "unique_labels = np.unique(D.label.values)\n",
    "\n",
    "## Removing Nones and obviously wrong super long lables\n",
    "unique_labels = [i for i in unique_labels if i is not None]\n",
    "unique_labels = [i for i in unique_labels if len(i)<900]\n",
    "\n",
    "print 'we have {} unique labels'.format(len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cats= np.unique(D['category'])\n",
    "##Create empty dictionary with categories as keys. We will use this to store part occurrence data for our categories\n",
    "label_vect_dict = {unique_cats[0]:None,unique_cats[1]:None,unique_cats[2]:None,unique_cats[3]:None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create vectors that contain the number of part instances in each sketch\n",
    "num_annots=3\n",
    "\n",
    "for category in unique_cats:\n",
    "    DS= D[D['category']==category]\n",
    "    unique_sketches_in_cat = np.unique(DS['sketch_id'])\n",
    "    unique_labels_in_cat = np.unique(DS['label'])\n",
    "    ## initialize matrix that has the correct dimensions\n",
    "    Label_Vec = np.zeros((len(unique_sketches_in_cat),len(unique_labels_in_cat)), dtype=int)\n",
    "    unique_labels_in_cat= np.array(unique_labels_in_cat)\n",
    "    for s,this_sketch in enumerate(unique_sketches_in_cat):\n",
    "        label_vec = np.zeros(len(unique_labels_in_cat),dtype=int)\n",
    "        DSS = DS[DS['sketch_id']==this_sketch]\n",
    "        annotation_ids = np.unique(DSS['annotation_id'].values)    \n",
    "        for this_annotation in annotation_ids:\n",
    "            DSA = DSS[DSS['annotation_id']==this_annotation]\n",
    "            label_list = DSA.label.values\n",
    "            for this_label in label_list:\n",
    "                label_ind = unique_labels_in_cat==this_label\n",
    "                label_vec[label_ind] += 1\n",
    "            \n",
    "        Label_Vec[s,:]=label_vec/num_annots\n",
    "    label_vect_dict[category]= Label_Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels=[]\n",
    "valid_labels_dict={}\n",
    "for category in unique_cats:\n",
    "    vect = label_vect_dict[category]\n",
    "    thresh = 50\n",
    "    #print 'These are the labels that appear at least {} times:'.format(thresh)\n",
    "    #print unique_labels[np.sum(Label_Vec,0)>thresh]\n",
    "    unique_labels_in_cat = np.unique(D[D['category']==category]['label'])\n",
    "    plot_labels= unique_labels_in_cat[np.sum(vect,0)>thresh]\n",
    "    valid_labels_dict[category]=plot_labels\n",
    "    valid_labels.append(plot_labels)\n",
    "\n",
    "\n",
    "    prop_labels=[]\n",
    "    for part in plot_labels:\n",
    "        DS=D[D['category']==category]\n",
    "        prop_labels.append(DS[DS['label']==part]['annotation_id'].nunique()/DS['annotation_id'].nunique())\n",
    "    \n",
    "##flattening valid labels\n",
    "valid_labels = [item for sublist in valid_labels for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a spline-level df where the modal label is set as the 'true' label for any given spline\n",
    "spline_df= D.groupby('spline_id').agg(lambda x: Counter(x).most_common(1)[0][0])\n",
    "spline_df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating a stroke-level dataframe that takes the mode value of annotation for its children splines to set as its\n",
    "##label value\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "stroke_svgs=OrderedDict()\n",
    "for category in unique_cats:\n",
    "    DS=D[D['category']==category]\n",
    "    for sketch in np.unique(DS['sketch_id']):\n",
    "        DSS=DS[DS['sketch_id']==sketch]\n",
    "        for stroke in np.unique(DSS['stroke_num']):\n",
    "            DSA=DSS[DSS['stroke_num']==stroke]\n",
    "            DSA=DSA.reset_index()\n",
    "            stroke_svgs[DSA['stroke_id'][0]] = DSA['sketch_svg_string'][0][stroke]\n",
    "\n",
    "            \n",
    "            \n",
    "stroke_svg_df= pd.DataFrame.from_dict(stroke_svgs, orient='index')    \n",
    "stroke_group_data= D.groupby('stroke_id').agg(lambda x: Counter(x).most_common(1)[0][0])\n",
    "labels= pd.DataFrame(stroke_group_data[['sketch_id','label','stroke_num','condition','target','category','outcome']])\n",
    "stroke_df=pd.merge(stroke_svg_df,labels,left_index=True, right_index =True)\n",
    "stroke_df.reset_index(level=0, inplace=True)\n",
    "stroke_df=stroke_df.rename(index=str, columns={\"index\": \"stroke_id\", 0: \"svg\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding total arclength information to stroke dataframe\n",
    "\n",
    "def calculate_arclength(svg):\n",
    "    try:\n",
    "        arclength= parse_path(svg).length()\n",
    "    except ZeroDivisionError:\n",
    "        print 'zero div error'\n",
    "        arclength = 0\n",
    "    return arclength\n",
    "stroke_df['arc_length'] = stroke_df['svg'].apply(calculate_arclength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesioning sketches test work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stroke_df_png = stroke_df.merge(png,how='right', on='sketch_id')\n",
    "\n",
    "# test_sketch = stroke_df_png[stroke_df_png['sketch_id']=='0647-bfcd78e5-085c-4631-a47c-0f3dadf71345_12']\n",
    "\n",
    "# parsed= test_sketch.svg\n",
    "\n",
    "# parsed= list(parsed)\n",
    "\n",
    "# for part in test_sketch.label.unique():\n",
    "#     test_sketch_l = test_sketch[test_sketch['label']!= part]\n",
    "#     parsed= list(test_sketch_l.svg)\n",
    "#     srh.render_svg(parsed,base_dir=sketch_dir,out_fname='without_{}.svg'.format(part))\n",
    "\n",
    "# svg_paths= srh.generate_svg_path_list(os.path.join(sketch_dir,'svg'))\n",
    "\n",
    "# srh.svg_to_png(svg_paths,base_dir=sketch_dir)\n",
    "\n",
    "# unique_sketches = stroke_df_png.sketch_id.unique()\n",
    "\n",
    "\n",
    "# rgbcols = sns.color_palette(\"husl\", len(chair_parts))\n",
    "# sns.palplot(rgbcols)\n",
    "\n",
    "# hexcols=[]\n",
    "# for this_col in rgbcols:\n",
    "#     hexcols.append(colors.to_hex(this_col))\n",
    "# hexcols= np.array(hexcols)\n",
    "\n",
    "# target_part= 'body'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this_chair in chair_df.sketch_id.unique():\n",
    "#     collist=[]\n",
    "#     chair_df_s = chair_df[chair_df['sketch_id']==this_chair]\n",
    "#     parts = chair_df_s.label\n",
    "#     for this_part in parts:\n",
    "#         if this_part == target_part:\n",
    "#             collist.append(hexcols[this_part==bird_parts][0])\n",
    "#         else:\n",
    "#             collist.append('#000000')\n",
    "#     svgstring = list(bird_dfs.svg)\n",
    "#     srh.render_svg_color(svgstring,base_dir=sketch_dir,stroke_colors=collist,out_fname='{}_{}_highlight.svg'.format(this_bird,target_part))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chair_df = stroke_df[stroke_df['category']=='chair']\n",
    "# chair_df = chair_df.reset_index()\n",
    "# chair_parts= stroke_df[stroke_df['category']=='chair'].label.unique()\n",
    "# chair_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_chairs = chair_df['sketch_id'].unique()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this_chair in sample_chairs:\n",
    "#     curr_chair = chair_df[chair_df['sketch_id']==this_chair]\n",
    "#     curr_chair_parts = curr_chair.label.unique()\n",
    "#     for this_part in chair_parts:\n",
    "#         if this_part in curr_chair_parts:\n",
    "#             les_sketch = curr_chair[curr_chair['label']!=this_part]\n",
    "#             paths= list(les_sketch.svg)\n",
    "#             srh.render_svg(paths,base_dir=sketch_dir,out_fname='{}_without_{}.svg'.format(this_chair,this_part))\n",
    "#         else:\n",
    "#             continue\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svg_paths= srh.generate_svg_path_list(os.path.join(sketch_dir,'svg'))\n",
    "# srh.svg_to_png(svg_paths,base_dir=sketch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this_sketch in sample_chairs:\n",
    "#     this_chair = chair_df[chair_df['sketch_id']==this_sketch]\n",
    "#     path = list(this_chair.svg)\n",
    "#     srh.render_svg(path,base_dir=sketch_dir,out_fname='{}_full.svg'.format(this_sketch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svg_paths= srh.generate_svg_path_list(os.path.join(sketch_dir,'svg'))\n",
    "# srh.svg_to_png(svg_paths,base_dir=sketch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = chair_df[chair_df['sketch_id'].isin(sample_chairs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create diagnostic lesions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we're lesioning all instances of the part with the overall highest arc length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem_chairs=[]\n",
    "# test_lesion_parts=[]\n",
    "# test_sketches=[]\n",
    "# for this_chair in chair_df.sketch_id.unique():\n",
    "#     this_sketch = chair_df.query('sketch_id == @this_chair')\n",
    "#     summed_al = pd.DataFrame(this_sketch.groupby('label').arc_length.agg(np.sum)).reset_index() ## df of parts with summed arclengths\n",
    "#     dpart = summed_al[summed_al.arc_length == summed_al.arc_length.max()].label ## part with highest overall arclength\n",
    "#     dpart = list(dpart)[0] ## dumb fix for df indexing issue\n",
    "#     les_sketch = this_sketch.query('label != @dpart')\n",
    "#     paths = list(les_sketch.svg) ## paths to use to render lesioned sketch\n",
    "#     if len(paths)==0:\n",
    "#         print(\"Lesion removes all paths in {}\".format(this_chair))\n",
    "#         problem_chairs.append(this_chair)\n",
    "#     else:\n",
    "#         test_lesion_parts.append(dpart)\n",
    "#         test_sketches.append(this_chair)\n",
    "#         srh.render_svg(paths,base_dir = test_dir,out_fname='{}.svg'.format(this_chair))\n",
    "    \n",
    "    \n",
    "    \n",
    "# svg_paths= srh.generate_svg_path_list(os.path.join(test_dir,'svg'))\n",
    "# srh.svg_to_png(svg_paths,base_dir=test_dir)\n",
    "# lesion_parts_meta = {'sketch_id':test_sketches, 'label':test_lesion_parts}\n",
    "# lesion_parts_meta = pd.DataFrame(data =lesion_parts_meta)\n",
    "\n",
    "# run=True\n",
    "# if run == True:\n",
    "#     lesion_parts_meta.to_csv(os.path.join(csv_dir,'test_lesion_meta.csv'))\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem_chairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create control lesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this_chair in chair_df.sketch_id.unique():\n",
    "#     if this_chair not in problem_chairs:\n",
    "#         this_sketch = chair_df.query('sketch_id == @this_chair')\n",
    "#         dpart = lesion_parts_meta.query('sketch_id==@this_chair').label\n",
    "#         dpart = list(dpart)[0]\n",
    "#         les_part = this_sketch.query('label==@dpart')\n",
    "#         amt_lesioned = np.sum(les_part.arc_length)\n",
    "#         rem_sketch = this_sketch.query('label!=@dpart')\n",
    "#         rem_paths = list(rem_sketch.svg)\n",
    "#         parsed_rem_paths = [parse_path(x) for x in rem_paths]\n",
    "#         shuffle(parsed_rem_paths)\n",
    "#         cc_path = concatpaths(parsed_rem_paths)\n",
    "#         if cc_path.length()>= amt_lesioned:\n",
    "#             rem_al = cc_path.ilength(cc_path.length()-amt_lesioned) ## keep only cc_path.length()-amt_lesioned worth of arclength\n",
    "#             rem_les = cc_path.cropped(0, rem_al) ##lesioning out the lesioned amount from remaining arc length\n",
    "#             les_part_paths =list(les_part.svg) ##paths for lesioned part\n",
    "#             srh.render_svg([rem_les,les_part_paths],base_dir = control_dir,out_fname='{}.svg'.format(this_chair)) ##add back lesioned part and render\n",
    "#         else:\n",
    "#             print (\"Lesion part too large to control\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svg_paths= srh.generate_svg_path_list(os.path.join(control_dir,'svg'))\n",
    "# srh.svg_to_png(svg_paths,base_dir=control_dir)\n",
    "# lesion_parts_meta = {'sketch_id':test_sketches, 'label':test_lesion_parts}\n",
    "# lesion_parts_meta = pd.DataFrame(data =lesion_parts_meta)\n",
    "\n",
    "# run=True\n",
    "# if run == True:\n",
    "#     lesion_parts_meta.to_csv(os.path.join(csv_dir,'test_lesion_meta.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferrring importance of stoke label on classifiability of lesioned sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chair_df = stroke_df[stroke_df['category']=='chair']\n",
    "chair_df = chair_df.reset_index()\n",
    "chair_parts= stroke_df[stroke_df['category']=='chair'].label.unique()\n",
    "chair_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pngs of intact sketches for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "really_run = False\n",
    "\n",
    "if really_run==True:\n",
    "\n",
    "    for this_chair in chair_df.sketch_id.unique():\n",
    "        curr_chair = chair_df.query('sketch_id == @this_chair')\n",
    "        svgs = list(curr_chair.svg)\n",
    "        srh.render_svg(svgs,base_dir=intact_dir,out_fname='{}.svg'.format(this_chair))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create path to lesioned svgs and convert to png for feature extraction\n",
    "really_run = False\n",
    "\n",
    "if really_run==True:\n",
    "    svg_paths= srh.generate_svg_path_list(os.path.join(intact_dir,'svg'))\n",
    "    srh.svg_to_png(svg_paths,base_dir=intact_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_feats = pd.DataFrame(np.load(os.path.join(vgg_dir,'FEATURES_FC6_sketch_no-channel-norm.npy')))\n",
    "intact_meta = pd.DataFrame(pd.read_csv(os.path.join(vgg_dir,'METADATA_sketch.csv')))\n",
    "assert intact_feats.shape[0]==intact_meta.shape[0]\n",
    "intact_df = pd.concat((intact_meta,intact_feats),axis=1)\n",
    "intact_df = intact_df.drop(columns= 'feature_ind')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create dataframe of additional sketch level metadata\n",
    "chair_trial_meta = chair_df.groupby('sketch_id')[['condition','category','target']].agg(pd.Series.mode)\n",
    "chair_trial_meta = chair_trial_meta.reset_index()\n",
    "intact_df = chair_trial_meta.join(intact_df.set_index('sketch_id'), on='sketch_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create new column for train test split stratification\n",
    "\n",
    "intact_df['strata'] = intact_df['condition'].astype(str) + '_' +intact_df['target'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create num_splits number of train test splits and get test accuracy for each split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "num_splits = 20 \n",
    "lc_list = []\n",
    "acc_list =[]\n",
    "for i in range(num_splits):\n",
    "    intact_df['strata'] = intact_df['condition'].astype(str) + '_' +intact_df['target'].astype(str)\n",
    "    train, test = train_test_split(intact_df, test_size=0.2, stratify=intact_df['strata'])\n",
    "    intact_df = intact_df.drop(columns='strata')\n",
    "    train = train.drop(columns = 'strata')\n",
    "    test = test.drop(columns = 'strata')\n",
    "    assert test.sketch_id.nunique()+train.sketch_id.nunique() == intact_df.sketch_id.nunique()\n",
    "    all_cols = list(intact_df.columns)\n",
    "    meta_cols = ['sketch_id','condition','category','target']\n",
    "\n",
    "    feat_cols = [x for x in all_cols if x not in meta_cols]\n",
    "    X_train = np.array(train[feat_cols])\n",
    "    Y_train = np.array(train['target'])\n",
    "\n",
    "\n",
    "    X_test = np.array(test[feat_cols])\n",
    "    Y_test = np.array(test['target'])\n",
    "    LC = LogisticRegression(solver='lbfgs', multi_class='multinomial',max_iter=100).fit(X_train, Y_train)\n",
    "    lc_list.append(LC)\n",
    "    Y_pred = LC.predict_proba(X_test)\n",
    "    print('Model log loss is : {}'.format(round(log_loss(Y_test,Y_pred),3)))\n",
    "    Y_class_pred =  LC.predict(X_test)\n",
    "    pred_df = pd.concat((test, pd.DataFrame(data = {\"prediction\":Y_class_pred==test.target})), axis=1)\n",
    "    acc = sum(pred_df.prediction)/pred_df.shape[0]\n",
    "    print('Accuracy is:{}'.format(acc))\n",
    "    acc_list.append(acc)\n",
    "\n",
    "    \n",
    "### Zipping together the different logistic classifiers with their associated test accuracy    \n",
    "val_list = zip(lc_list, acc_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###extract best performing classifier \n",
    "\n",
    "c= [x[1] for x in val_list]\n",
    "d = np.array(c).max()\n",
    "e= c.index(d)\n",
    "val_list[e][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print('Naive log loss would be : {}'.format(round(-math.log(1/8),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('target').agg('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby('target').agg('nunique')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create k lesion-sketches per intact sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each sketch create k lesioned sketches where k is the number of strokes in the sketch; each lesioned sketch\\\n",
    "## has one of the k strokes removed. Total number of sketches should be equal to total number of strokes in dataset\n",
    "\n",
    "meta_labels = []\n",
    "meta_arclength = []\n",
    "meta_conds = []\n",
    "meta_target = []\n",
    "meta_objects = []\n",
    "meta_categories = []\n",
    "meta_les_ids = []\n",
    "meta_sketch_ids = []\n",
    "for this_chair in chair_df['sketch_id'].unique():\n",
    "    this_sketch = chair_df.query('sketch_id == @this_chair')\n",
    "    if this_sketch.stroke_num.nunique()<2:\n",
    "        print ('single stroke sketch- {}'.format(this_chair))\n",
    "        continue \n",
    "    intact_paths = list(this_sketch.svg)\n",
    "    stroke_labels = this_sketch.label\n",
    "    for this_stroke in this_sketch.stroke_num:\n",
    "        this_lesion = this_sketch.query('stroke_num == @this_stroke')\n",
    "        les_stroke_path = list(this_lesion.svg)\n",
    "\n",
    "        meta_labels.append(this_lesion.label.iloc[0])\n",
    "        meta_arclength.append(parse_path(les_stroke_path[0]).length())\n",
    "        meta_conds.append(this_lesion.condition.iloc[0])\n",
    "        meta_target.append(this_lesion.target.iloc[0])\n",
    "        meta_objects.append(this_lesion.target.iloc[0])\n",
    "        meta_categories.append(this_lesion.category.iloc[0])\n",
    "\n",
    "        les_id = str(this_lesion.sketch_id.iloc[0])+'_'+str(this_lesion.label.iloc[0])+'_'+str(this_lesion.stroke_num.iloc[0])\n",
    "        meta_les_ids.append(les_id)\n",
    "        meta_sketch_ids.append(this_lesion.sketch_id.iloc[0])\n",
    "        les_paths = [x for x in intact_paths if x not in les_stroke_path]\n",
    "        really_run = False\n",
    "        if really_run==True:\n",
    "            srh.render_svg(les_paths,base_dir = lesion_dir,out_fname='{}.svg'.format(les_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create path to lesioned svgs and convert to png for feature extraction\n",
    "really_run = False\n",
    "\n",
    "if really_run==True:\n",
    "    svg_paths= srh.generate_svg_path_list(os.path.join(lesion_dir,'svg'))\n",
    "    srh.svg_to_png(svg_paths,base_dir=lesion_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create meta dataframe for the lesioned sketches sketches\n",
    "les_chair_meta = {'lesion_id':meta_les_ids, 'label':meta_labels,'target':meta_target, 'condition':meta_conds, 'category':meta_categories,\\\n",
    "                 'sketch_id':meta_sketch_ids, 'arc_length':meta_arclength}\n",
    "lesion_parts_meta = pd.DataFrame(data =les_chair_meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in lesion features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesioned_feats = pd.DataFrame(np.load(os.path.join(vgg_dir,'FEATURES_FC6_sketch_channel-norm_lesioned.npy')))#\n",
    "lesioned_meta = pd.DataFrame(pd.read_csv(os.path.join(vgg_dir,'METADATA_sketch_lesioned.csv')))\n",
    "assert lesioned_feats.shape[0]==lesion_parts_meta.shape[0]==lesioned_meta.shape[0]\n",
    "### Concatenate feature columns with 'lesion_id' column (labeled as sketch_id in lesioned_meta)\n",
    "lesioned_df = pd.concat((lesioned_meta,lesioned_feats),axis=1).drop(columns = ['feature_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### concat additional metadata df with lesioned_df and do some index resetting\n",
    "\n",
    "lesioned_df = lesioned_df.set_index('sketch_id').join(lesion_parts_meta.set_index('lesion_id'))\n",
    "lesioned_df.index.names = ['lesion_id']\n",
    "lesioned_df = lesioned_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to calculate the classifiability score of a lesion, a.k.a. the class probability assigned to\\\n",
    "### the true class label for that sketch by the classifier\n",
    "lc_classes = LC.classes_\n",
    "def calc_class_score(df):\n",
    "    df=df.to_frame().T.reset_index()\n",
    "    features = df[feat_cols]\n",
    "    target = df['target'].iloc[0]\n",
    "    feats = LC.predict_proba(features).reshape(8,)   \n",
    "    \n",
    "    correct_pos = feats[lc_classes == target]\n",
    "   \n",
    "    return(correct_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add classifiability score to the dataframe\n",
    "\n",
    "lesioned_df['c_score'] = lesioned_df.apply(calc_class_score,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save out csv. TODO : Drop feature columns\n",
    "\n",
    "lesioned_df.to_csv(os.path.join(csv_dir,'lesion_sketch_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
