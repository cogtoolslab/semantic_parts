{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import urllib, cStringIO\n",
    "\n",
    "import pymongo as pm\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from svgpathtools import parse_path\n",
    "import svgpathtools\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "import sys\n",
    "\n",
    "from svgpathtools import parse_path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import svg_rendering_helpers as srh\n",
    "if sys.version_info[0]>=3:\n",
    "    from importlib import reload\n",
    "reload(srh)\n",
    "\n",
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('../..')\n",
    "analysis_dir = os.getcwd()\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "features_dir= os.path.join(results_dir,'features')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'experiments'))\n",
    "sketch_dir = os.path.abspath(os.path.join(proj_dir,'sketches'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'analysis'))\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)  \n",
    "\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)  \n",
    "\n",
    "if not os.path.exists(features_dir):\n",
    "    os.makedirs(features_dir)\n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'analysis'))        \n",
    "    \n",
    "# Assign variables within imported analysis helpers\n",
    "import analysis_helpers as h\n",
    "if sys.version_info[0]>=3:\n",
    "    from importlib import reload\n",
    "reload(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper dictionaries \n",
    "OBJECT_TO_CATEGORY = {\n",
    "    'basset': 'dog', 'beetle': 'car', 'bloodhound': 'dog', 'bluejay': 'bird',\n",
    "    'bluesedan': 'car', 'bluesport': 'car', 'brown': 'car', 'bullmastiff': 'dog',\n",
    "    'chihuahua': 'dog', 'crow': 'bird', 'cuckoo': 'bird', 'doberman': 'dog',\n",
    "    'goldenretriever': 'dog', 'hatchback': 'car', 'inlay': 'chair', 'knob': 'chair',\n",
    "    'leather': 'chair', 'nightingale': 'bird', 'pigeon': 'bird', 'pug': 'dog',\n",
    "    'redantique': 'car', 'redsport': 'car', 'robin': 'bird', 'sling': 'chair',\n",
    "    'sparrow': 'bird', 'squat': 'chair', 'straight': 'chair', 'tomtit': 'bird',\n",
    "    'waiting': 'chair', 'weimaraner': 'dog', 'white': 'car', 'woven': 'chair',\n",
    "}\n",
    "CATEGORY_TO_OBJECT = {\n",
    "    'dog': ['basset', 'bloodhound', 'bullmastiff', 'chihuahua', 'doberman', 'goldenretriever', 'pug', 'weimaraner'],\n",
    "    'car': ['beetle', 'bluesedan', 'bluesport', 'brown', 'hatchback', 'redantique', 'redsport', 'white'],\n",
    "    'bird': ['bluejay', 'crow', 'cuckoo', 'nightingale', 'pigeon', 'robin', 'sparrow', 'tomtit'],\n",
    "    'chair': ['inlay', 'knob', 'leather', 'sling', 'squat', 'straight', 'waiting', 'woven'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Helpers\n",
    "\n",
    "\n",
    "\n",
    "def subset_dataframe_by_condition(F,to_inspect='all',this_category='bird',this_object='bluejay'):\n",
    "    '''\n",
    "    input: F: dataframe (num_sketches x num_features)\n",
    "           to_inspect: a string indicating whether to subset by ['object','category','all']\n",
    "           this_category: IF to_inspect == 'category', then we define this to subset by that category only\n",
    "           this_object: IF to_inspect == 'object', then we define this to subset by that object only\n",
    "           \n",
    "    returns: two feature matrices, c and f, corresponding to the close and far subsetted feature matrices\n",
    "           \n",
    "    '''\n",
    "        \n",
    "    F = F.sort_values(by=['category','target'])\n",
    "\n",
    "    ## get context condition inds for subsetting dataframe\n",
    "    close_inds = F['condition'] == 'closer'\n",
    "    far_inds = F['condition'] == 'further'\n",
    "\n",
    "    ## if we want to inspect particular category\n",
    "    category_inds = F['category']==this_category\n",
    "\n",
    "    ## if we want to inspect particular object\n",
    "    obj_list = np.unique(F.target.values)\n",
    "    obj_inds = F['target']==this_object  \n",
    "    \n",
    "    ## get names of columns that contain stroke-count & arclength information\n",
    "    numstrokes_cols = [i for i in F.columns if i.split('_')[-1]=='numstrokes']\n",
    "    arclength_cols = [i for i in F.columns if i.split('_')[-1]=='arclength']\n",
    "    feat_cols = numstrokes_cols + arclength_cols\n",
    "    \n",
    "    if to_inspect == 'object':    \n",
    "        ## extract particular row corresponding to this OBJECT in each condition\n",
    "        f = F[(far_inds) & obj_inds][feat_cols].reset_index(drop=True)\n",
    "        c = F[(close_inds) & obj_inds][feat_cols].reset_index(drop=True)\n",
    "        obj_listf = F[(far_inds) & obj_inds]['target'].values\n",
    "        obj_listc = F[(close_inds) & obj_inds]['target'].values\n",
    "    elif to_inspect == 'category':\n",
    "        ## extract particular rows corresponding to this CATEGORY in each condition\n",
    "        f = F[(category_inds) & (far_inds)][feat_cols].reset_index(drop=True)\n",
    "        c = F[(category_inds) & (close_inds)][feat_cols].reset_index(drop=True)\n",
    "        obj_listf = F[(category_inds) & (far_inds)]['target'].values\n",
    "        obj_listc = F[(category_inds) & (close_inds)]['target'].values\n",
    "    elif to_inspect == 'all':\n",
    "        ## extract particular rows corresponding to each condition\n",
    "        f = F[far_inds][feat_cols].reset_index(drop=True)\n",
    "        c = F[close_inds][feat_cols].reset_index(drop=True) \n",
    "        obj_listf = F[far_inds]['target'].values\n",
    "        obj_listc = F[close_inds]['target'].values\n",
    "        \n",
    "    return c, f, obj_listc, obj_listf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in annotated sketch dataset| subsetted for sketches with 3 annotations\n",
    "D = pd.read_pickle(os.path.join(csv_dir, 'semantic_parts_annotated_data_pckl'))\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png= pd.read_csv(os.path.join(csv_dir, 'semantic_parts_annotated_pngstring.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the list of unique labels applied to sketches\n",
    "unique_labels = np.unique(D.label.values)\n",
    "\n",
    "## Removing Nones and obviously wrong super long lables\n",
    "unique_labels = [i for i in unique_labels if i is not None]\n",
    "unique_labels = [i for i in unique_labels if len(i)<900]\n",
    "\n",
    "print 'we have {} unique labels'.format(len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cats= np.unique(D['category'])\n",
    "##Create empty dictionary with categories as keys. We will use this to store part occurrence data for our categories\n",
    "label_vect_dict = {unique_cats[0]:None,unique_cats[1]:None,unique_cats[2]:None,unique_cats[3]:None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create vectors that contain the number of part instances in each sketch\n",
    "num_annots=3\n",
    "\n",
    "for category in unique_cats:\n",
    "    DS= D[D['category']==category]\n",
    "    unique_sketches_in_cat = np.unique(DS['sketch_id'])\n",
    "    unique_labels_in_cat = np.unique(DS['label'])\n",
    "    ## initialize matrix that has the correct dimensions\n",
    "    Label_Vec = np.zeros((len(unique_sketches_in_cat),len(unique_labels_in_cat)), dtype=int)\n",
    "    unique_labels_in_cat= np.array(unique_labels_in_cat)\n",
    "    for s,this_sketch in enumerate(unique_sketches_in_cat):\n",
    "        label_vec = np.zeros(len(unique_labels_in_cat),dtype=int)\n",
    "        DSS = DS[DS['sketch_id']==this_sketch]\n",
    "        annotation_ids = np.unique(DSS['annotation_id'].values)    \n",
    "        for this_annotation in annotation_ids:\n",
    "            DSA = DSS[DSS['annotation_id']==this_annotation]\n",
    "            label_list = DSA.label.values\n",
    "            for this_label in label_list:\n",
    "                label_ind = unique_labels_in_cat==this_label\n",
    "                label_vec[label_ind] += 1\n",
    "            \n",
    "        Label_Vec[s,:]=label_vec/num_annots\n",
    "    label_vect_dict[category]= Label_Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels=[]\n",
    "valid_labels_dict={}\n",
    "for category in unique_cats:\n",
    "    vect = label_vect_dict[category]\n",
    "    thresh = 50\n",
    "    #print 'These are the labels that appear at least {} times:'.format(thresh)\n",
    "    #print unique_labels[np.sum(Label_Vec,0)>thresh]\n",
    "    unique_labels_in_cat = np.unique(D[D['category']==category]['label'])\n",
    "    plot_labels= unique_labels_in_cat[np.sum(vect,0)>thresh]\n",
    "    valid_labels_dict[category]=plot_labels\n",
    "    valid_labels.append(plot_labels)\n",
    "\n",
    "\n",
    "    prop_labels=[]\n",
    "    for part in plot_labels:\n",
    "        DS=D[D['category']==category]\n",
    "        prop_labels.append(DS[DS['label']==part]['annotation_id'].nunique()/DS['annotation_id'].nunique())\n",
    "    \n",
    "##flattening valid labels\n",
    "valid_labels = [item for sublist in valid_labels for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a spline-level df where the modal label is set as the 'true' label for any given spline\n",
    "spline_df= D.groupby('spline_id').agg(lambda x: Counter(x).most_common(1)[0][0])\n",
    "spline_df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating a stroke-level dataframe that takes the mode value of annotation for its children splines to set as its\n",
    "##label value\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "stroke_svgs=OrderedDict()\n",
    "for category in unique_cats:\n",
    "    DS=D[D['category']==category]\n",
    "    for sketch in np.unique(DS['sketch_id']):\n",
    "        DSS=DS[DS['sketch_id']==sketch]\n",
    "        for stroke in np.unique(DSS['stroke_num']):\n",
    "            DSA=DSS[DSS['stroke_num']==stroke]\n",
    "            DSA=DSA.reset_index()\n",
    "            stroke_svgs[DSA['stroke_id'][0]] = DSA['sketch_svg_string'][0][stroke]\n",
    "\n",
    "            \n",
    "            \n",
    "stroke_svg_df= pd.DataFrame.from_dict(stroke_svgs, orient='index')    \n",
    "stroke_group_data= D.groupby('stroke_id').agg(lambda x: Counter(x).most_common(1)[0][0])\n",
    "labels= pd.DataFrame(stroke_group_data[['sketch_id','label','stroke_num','condition','target','category','outcome']])\n",
    "stroke_df=pd.merge(stroke_svg_df,labels,left_index=True, right_index =True)\n",
    "stroke_df.reset_index(level=0, inplace=True)\n",
    "stroke_df=stroke_df.rename(index=str, columns={\"index\": \"stroke_id\", 0: \"svg\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding total arclength information to stroke dataframe\n",
    "\n",
    "def calculate_arclength(svg):\n",
    "    try:\n",
    "        arclength= parse_path(svg).length()\n",
    "    except ZeroDivisionError:\n",
    "        print 'zero div error'\n",
    "        arclength = 0\n",
    "    return arclength\n",
    "stroke_df['arc_length'] = stroke_df['svg'].apply(calculate_arclength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df_png = stroke_df.merge(png,how='right', on='sketch_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sketch = stroke_df_png[stroke_df_png['sketch_id']=='0647-bfcd78e5-085c-4631-a47c-0f3dadf71345_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed= test_sketch.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed= list(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in test_sketch.label.unique():\n",
    "    test_sketch_l = test_sketch[test_sketch['label']!= part]\n",
    "    parsed= list(test_sketch_l.svg)\n",
    "    srh.render_svg(parsed,base_dir=sketch_dir,out_fname='without_{}.svg'.format(part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svg_paths= srh.generate_svg_path_list(os.path.join(sketch_dir,'svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svg_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srh.svg_to_png(svg_paths,base_dir=sketch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(srh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
