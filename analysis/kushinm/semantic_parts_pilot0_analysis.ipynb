{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import urllib, cStringIO\n",
    "\n",
    "import pymongo as pm\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "import sys\n",
    "\n",
    "from svgpathtools import parse_path\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and creating dataframe for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('../..')\n",
    "analysis_dir = os.getcwd()\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'experiments'))\n",
    "sketch_dir = os.path.abspath(os.path.join(proj_dir,'sketches'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'analysis'))\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)  \n",
    "\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)  \n",
    "    \n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'analysis'))        \n",
    "    \n",
    "# Assign variables within imported analysis helpers\n",
    "import analysis_helpers as h\n",
    "if sys.version_info[0]>=3:\n",
    "    from importlib import reload\n",
    "reload(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting up connection to mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### set vars \n",
    "auth = pd.read_csv('auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "key  = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'rxdhawkins.me' ## cocolab ip address\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "import pymongo as pm\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "db = conn['semantic_parts']\n",
    "coll = db['sketchpad_basic']\n",
    "\n",
    "# which iteration name should we use?\n",
    "iterationName = 'pilot0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sketches = coll.find({'iterationName':iterationName}).count()\n",
    "print 'We have {} annotations so far.'.format(num_sketches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jefan = ['A1MMCS8S8CTWKU','A1MMCS8S8CTWKV','A1MMCS8S8CTWKS']\n",
    "hawkrobe = ['A1BOIDKD33QSDK']\n",
    "kmukherjee = ['A1WU4IHJNQGVAY']\n",
    "researchers = jefan + hawkrobe  + kmukherjee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_assignments = coll.find({'iterationName':iterationName}).distinct('aID')\n",
    "print 'We have had {} unique sessions'.format( len(unique_assignments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get list of unique_assignments\n",
    "unique_assignments = coll.find({'iterationName':iterationName}).distinct('aID')\n",
    "\n",
    "### initialize a bunch of stuff\n",
    "orig_gameID = [] # the gameID from which this sketch was sourced\n",
    "orig_trial_num = [] # the trialnum in the original game from which this sketch was sourced -- \n",
    "sketch_id = [] # concatenation of orig_gameID and orig_trial_num -- \n",
    "assignmentID = [] # the session in which this annotation was collected -- \n",
    "annotation_id = [] # the unique ID for each annotation trial (different for each session the same sketch appears in)\n",
    "category = [] # e.g., \"chair\"\n",
    "target = [] # e.g., \"inlay\"\n",
    "condition = [] # e.g., \"closer\" vs. \"further\" or \"repeated\" vs. \"control\n",
    "trial_num = [] \n",
    "workerID = [] #mTurk workerId\n",
    "spline_id =[] #unique spline identifier\n",
    "time_submitted = [] # when the participant clicked \"next sketch\"\n",
    "time_labeled = [] # unique to each spline labeled\n",
    "time_clicked = [] # when this spline was clicked/selected\n",
    "num_strokes_in_sketch = [] # how many strokes in this sketch\n",
    "num_splines_in_sketch = [] # how many spline elements in this sketch\n",
    "stroke_num = [] # which stroke number this labeled spline came from\n",
    "cumulative_spline_num = [] # spline index in the cumulative spline sequence for the entire sketch\n",
    "within_stroke_spline_num = [] # spline index for the current stroke\n",
    "cumulative_bout_num= [] #which bout of annotation the spline belonged to\n",
    "part_bout_num =[] #which part-specific bout of annotation the spline belonged to\n",
    "label = [] # the label provided by the participant\n",
    "spline_svg_string = [] # the svg spline string that earned this label\n",
    "sketch_svg_string = [] # the entire svg string correponding to this sketch\n",
    "annotation_flag = [] # this is True if all splines were labeled as the same thing\n",
    "annotation_spline_id = [] #unique identifier for specific annotation of a spline\n",
    "png=[] #png string for the annotated sketch\n",
    "stroke_id=[]\n",
    "\n",
    "## loop through all the unique assignments that have submitted things\n",
    "for this_assignment, aID in enumerate(unique_assignments):\n",
    "    if this_assignment%10==0:\n",
    "        print 'Analyzing sketches from assignment {} of {}  ...'.format(this_assignment, len(unique_assignments))\n",
    "\n",
    "    ### get all the sketch recs for this assignment\n",
    "    sketch_recs = coll.find({'$and': [{'iterationName':iterationName}, {'aID':aID}]}).sort('time')\n",
    "\n",
    "    try:\n",
    "\n",
    "        for sketch_ind,sketch in enumerate(sketch_recs):\n",
    "            ## get annotations embedded within record\n",
    "            sketch_cat = sketch['category']\n",
    "            annotations_string = sketch['annotations']\n",
    "    \n",
    "            ## convert to json dictionary\n",
    "            _annotations_dict = json.loads(annotations_string)  \n",
    "           \n",
    "            annotations_dict = _annotations_dict[0][sketch_cat]\n",
    "            png_string = _annotations_dict[0]['png']\n",
    "            num_splines = len(annotations_dict)\n",
    "            for annotation in annotations_dict:\n",
    "                assert sketch['numSplines']==num_splines                \n",
    "                ## get spline-level metadata\n",
    "            \n",
    "                workerID.append(h.encode(key,sketch['wID']))\n",
    "                label.append(annotation['label'])\n",
    "                stroke_num.append(annotation['strokeNum'])\n",
    "                spline_svg_string.append(annotation['svgString'])\n",
    "                cumulative_spline_num.append(annotation['cumulativeSplineNum'])\n",
    "                within_stroke_spline_num.append(annotation['withinStrokeSplineNum'])\n",
    "                time_clicked.append(annotation['timeClicked'])\n",
    "                time_labeled.append(annotation['timeLabeled'])\n",
    "                spline_id.append('{}_{}_{}'.format(sketch['originalGameID'],sketch['originalTrialNum'],annotation['cumulativeSplineNum']))\n",
    "                stroke_id.append('{}_{}_{}'.format(sketch['originalGameID'],sketch['originalTrialNum'],annotation['strokeNum']))\n",
    "                cumulative_bout_num.append(annotation['boutNum'])\n",
    "                part_bout_num.append(annotation['partBoutNum'])\n",
    "                ## get sketch-level metadata\n",
    "                orig_gameID.append(sketch['originalGameID'])   \n",
    "                orig_trial_num.append(sketch['originalTrialNum'])\n",
    "                sketch_id.append('{}_{}'.format(sketch['originalGameID'],sketch['originalTrialNum']))\n",
    "                annotation_id.append('{}_{}_{}'.format(sketch['originalGameID'],sketch['originalTrialNum'],sketch['aID']))\n",
    "                assignmentID.append(sketch['aID'])\n",
    "                category.append(sketch['category'])\n",
    "                target.append(sketch['target'])\n",
    "                png.append(png_string)\n",
    "                condition.append(sketch['condition'])\n",
    "                time_submitted.append(sketch['time'])\n",
    "                trial_num.append(sketch['trialNum'])\n",
    "                num_splines_in_sketch.append(sketch['numSplines'])\n",
    "                num_strokes_in_sketch.append(sketch['numStrokes'])\n",
    "                sketch_svg_string.append(sketch['svg'])\n",
    "                annotation_flag.append(sketch['sameAnnotflag'])\n",
    "                annotation_spline_id.append('{}_{}_{}_{}'.format(sketch['originalGameID'],sketch['originalTrialNum'],sketch['aID'],annotation['cumulativeSplineNum']))\n",
    "                \n",
    "    except AssertionError:\n",
    "        print 'There were unequal numbers for sketch[\"numSplines\"] vs. num_splines for sketch {} from {}'.\\\n",
    "                format(sketch['trialNum'], sketch['aID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make group dataframe \n",
    "D = pd.DataFrame([workerID,orig_gameID, orig_trial_num, sketch_id, category, assignmentID,  target, \\\n",
    "                  annotation_id, condition, trial_num, time_submitted,\\\n",
    "                 time_labeled, time_clicked, num_strokes_in_sketch, num_splines_in_sketch,\\\n",
    "                 stroke_num, cumulative_spline_num, within_stroke_spline_num, cumulative_bout_num,\\\n",
    "                 part_bout_num, label, spline_svg_string, sketch_svg_string, spline_id, stroke_id,\\\n",
    "                  annotation_spline_id,png])\n",
    "D = D.transpose()\n",
    "D.columns = ['workerID','orig_gameID', 'orig_trial_num', 'sketch_id', 'category', 'assignmentID', 'target',\\\n",
    "             'annotation_id', 'condition', 'trial_num', 'time_submitted',\\\n",
    "             'time_labeled', 'time_clicked', 'num_strokes_in_sketch', 'num_splines_in_sketch',\\\n",
    "             'stroke_num', 'cumulative_spline_num', 'within_stroke_spline_num', 'cumulative_bout_num', 'part_bout_num', 'label',\\\n",
    "             'spline_svg_string', 'sketch_svg_string', 'spline_id','stroke_id','annotation_spline_id','png']\n",
    "D=D[D['assignmentID']!='']\n",
    "\n",
    "\n",
    "print 'Annotations dataframe contains {} rows and {} columns.'.format(D.shape[0],D.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check to see what dataframe looks like\n",
    "D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in D.iterrows():\n",
    "    if row['label'] is None:\n",
    "        row['label'] = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations and desriptive statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a count of how many unique sketches have been annotated\n",
    "\n",
    "unique_sketches = np.unique(D['sketch_id'].values)\n",
    "print 'We have {} unique sketches.'.format(len(unique_sketches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of annotations per sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get number of times each sketch has been annotated\n",
    "num_times_annotated = []\n",
    "for this_sketch_id in unique_sketches:\n",
    "    num_times_annotated.append(D[D['sketch_id']==this_sketch_id]['assignmentID'].nunique())\n",
    "    \n",
    "## make a histogram\n",
    "sns.set_context('talk')\n",
    "plt.figure(figsize=(6,5))\n",
    "h = plt.hist(num_times_annotated)\n",
    "plt.xticks(np.arange(0, 4, step=1))\n",
    "plt.title('Times each sketch has been annotated')\n",
    "plt.ylabel('number of sketches')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at time taken to annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Make sure the number of splines for each sketch is consistent across annotations\n",
    "for this_sketch in unique_sketches:\n",
    "    assert len(np.unique(D[D['sketch_id']==this_sketch]['num_splines_in_sketch'].values))==1\n",
    "\n",
    "unique_annotation_trials = np.unique(D['annotation_id'].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get annotation time for each annotation trial\n",
    "annotation_time = []\n",
    "spline_number_in_sketch = []\n",
    "\n",
    "for this_annotation_trial in unique_annotation_trials:\n",
    "    earliest_click = float(np.min(D[D['annotation_id']==this_annotation_trial]['time_clicked']))\n",
    "    ## all of the splines were submitted at the same time, so time_submitted should be identical for all splines in an annotation trial\n",
    "    assert len(np.unique(D[D['annotation_id']==this_annotation_trial]['time_submitted'].values))==1\n",
    "    final_submission = np.unique(D[D['annotation_id']==this_annotation_trial]['time_submitted'].values)[0]\n",
    "    annotation_time.append(final_submission-earliest_click)        \n",
    "    if np.isnan(final_submission) or np.isnan(earliest_click):\n",
    "        print 'One of these timestamps is a NaN. Probably means that the participant skipped this trial:'\n",
    "        print this_annotation_trial\n",
    "        print 'final_submission: {}, earliest_click: {}'.format(final_submission, earliest_click)\n",
    "    #### then we will extract how \"complex\" each sketch i\n",
    "    assert len(np.unique(D[D['annotation_id']==this_annotation_trial]['num_splines_in_sketch'].values))==1    \n",
    "    spline_number_in_sketch.append(np.unique(D[D['annotation_id']==this_annotation_trial]['num_splines_in_sketch'])[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert annotation time to seconds\n",
    "annotation_time_seconds = np.array(annotation_time)/1000\n",
    "\n",
    "## make dataframe with annotation time and spline number\n",
    "unique_annotation_trials, spline_number_in_sketch, annotation_time_seconds = map(list, [unique_annotation_trials, spline_number_in_sketch, annotation_time_seconds])\n",
    "T = pd.DataFrame([unique_annotation_trials,spline_number_in_sketch,annotation_time_seconds])\n",
    "T = T.transpose()\n",
    "T.columns = ['annotation_trial','spline_number_in_sketch','annotation_time']\n",
    "\n",
    "## some preprocessing of T\n",
    "import analysis_helpers as h\n",
    "if sys.version_info[0]>=3:\n",
    "    from importlib import reload\n",
    "reload(h)\n",
    "\n",
    "\n",
    "## make numeric types\n",
    "T = h.convert_numeric(T,'spline_number_in_sketch')\n",
    "T = h.convert_numeric(T,'annotation_time')\n",
    "\n",
    "## also remove the skipped trial where annotation time is a NaN\n",
    "T = T[~np.isnan(T['annotation_time'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make scatterplot of relationship between annotation time and spline number\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x='annotation_time',\n",
    "                y='spline_number_in_sketch',\n",
    "                data=T)\n",
    "plt.ylabel('number of splines in sketch')\n",
    "plt.xlabel('annotation time (s)')\n",
    "plt.title('Do more complex sketches take longer to annotate?')\n",
    "plt.xlim(0,60*5) ## 5 minute cutoff ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p = stats.spearmanr(T['annotation_time'],T['spline_number_in_sketch'])\n",
    "print 'Spearman correlation between annotation time and spline number in sketch' \n",
    "print 'r = {}, p = {}'.format(r,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at part occurrence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the list of unique labels applied to sketches\n",
    "unique_labels = np.unique(D.label.values)\n",
    "\n",
    "## Removing Nones and obviously wrong super long lables\n",
    "unique_labels = [i for i in unique_labels if i is not None]\n",
    "unique_labels = [i for i in unique_labels if len(i)<900]\n",
    "\n",
    "print 'we have {} unique labels'.format(len( unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get a list of categories\n",
    "unique_cats = np.unique(D['category'])\n",
    "\n",
    "##Create empty dictionary with categories as keys. We will use this to store part occurrence data for our categories\n",
    "label_vect_dict = {unique_cats[0]:None,unique_cats[1]:None,unique_cats[2]:None,unique_cats[3]:None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create vectors that contain the number of part instances in each sketch\n",
    "\n",
    "for category in unique_cats:\n",
    "    DS= D[D['category']==category]\n",
    "    unique_sketches_in_cat = np.unique(DS['sketch_id'])\n",
    "    unique_labels_in_cat = np.unique(DS['label'])\n",
    "    ## initialize matrix that has the correct dimensions\n",
    "    Label_Vec = np.zeros((len(unique_sketches_in_cat),len(unique_labels_in_cat)), dtype=int)\n",
    "    unique_labels_in_cat= np.array(unique_labels_in_cat)\n",
    "    for s,this_sketch in enumerate(unique_sketches_in_cat):\n",
    "        label_vec = np.zeros(len(unique_labels_in_cat),dtype=int)\n",
    "        DSS = DS[DS['sketch_id']==this_sketch]\n",
    "        annotation_ids = np.unique(DS['annotation_id'].values)    \n",
    "        for this_annotation in annotation_ids:\n",
    "            DSA = DSS[DSS['annotation_id']==this_annotation]\n",
    "            label_list = np.unique(DSA.label.values)\n",
    "            for this_label in label_list:\n",
    "                label_ind = unique_labels_in_cat==this_label\n",
    "                label_vec[label_ind] += 1\n",
    "            \n",
    "        Label_Vec[s,:]=label_vec\n",
    "    label_vect_dict[category]= Label_Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vect_dict['bird'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(plot_labels[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels=[]\n",
    "for category in unique_cats:\n",
    "    vect = label_vect_dict[category]\n",
    "    thresh = 100\n",
    "    #print 'These are the labels that appear at least {} times:'.format(thresh)\n",
    "    #print unique_labels[np.sum(Label_Vec,0)>thresh]\n",
    "    unique_labels_in_cat = np.unique(D[D['category']==category]['label'])\n",
    "    plot_labels= unique_labels_in_cat[np.sum(vect,0)>thresh]\n",
    "    valid_labels.append(plot_labels)\n",
    "\n",
    "#CHECK\n",
    "    prop_labels=[]\n",
    "    for part in plot_labels:\n",
    "        DS=D[D['category']==category]\n",
    "        prop_labels.append(DS[DS['label']==part]['annotation_id'].nunique()/DS['annotation_id'].nunique())\n",
    "    \n",
    "    \n",
    "    sns.set_context('talk')\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.ylim(0,1)\n",
    "    h = plt.bar(plot_labels,prop_labels)\n",
    "    plt.title('Proportion of {} annotations with labels'.format(category))\n",
    "    plt.ylabel('proportion of annotations')\n",
    "    plt.xlabel('Part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = [item for sublist in valid_labels for item in sublist]\n",
    "valid_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Getting ride of splines that have 'None' as labels - splines that were not annotated\n",
    "D=D[D['label'].isin(valid_labels)]\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving out the PNG for the sketches\n",
    "from matplotlib.pyplot import imshow\n",
    "import base64\n",
    "\n",
    "\n",
    "#num_diff_annots = []\n",
    "for this_sketch_id in unique_sketches:\n",
    "    DS=D[D['sketch_id']==this_sketch_id]\n",
    "    unique_splines = np.unique(DS['cumulative_spline_num'])\n",
    "    for i,this_spline in enumerate(unique_splines):\n",
    "        DSS =DS[DS['cumulative_spline_num']==this_spline]\n",
    "        num_diff_annots= len(np.unique(DSS['label']))\n",
    "    if num_diff_annots>0:  ##Will update this conditional once we have more annots\n",
    "        for instance in np.unique(DS['annotation_id']):\n",
    "                imgdata = base64.b64decode(DS[DS['annotation_id']==instance].iloc[0]['png'])\n",
    "                filename = '{}_{}'.format(instance, DS[DS['annotation_id']==instance].iloc[0]['target'])  # I assume you have a way of picking unique filenames\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(imgdata)\n",
    "                im = Image.open(filename)\n",
    "                plt.figure()\n",
    "                imshow(im)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                #plt.savefig(os.path.join(plot_dir,'{}_{}'.format(instance, DS[DS['annotation_id']==instance].iloc[0]['target'])),edgecolor='w',bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making part-transition matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating a stroke-level dataframe that takes the mode value of annotation for its children splines to set as its\n",
    "##label value\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "stroke_svgs=OrderedDict()\n",
    "for category in unique_cats:\n",
    "    DS=D[D['category']==category]\n",
    "    for sketch in np.unique(DS['sketch_id']):\n",
    "        DSS=DS[DS['sketch_id']==sketch]\n",
    "        for stroke in np.unique(DSS['stroke_num']):\n",
    "            DSA=DSS[DSS['stroke_num']==stroke]\n",
    "            DSA=DSA.reset_index()\n",
    "            stroke_svgs[DSA['stroke_id'][0]] = DSA['sketch_svg_string'][0][stroke]\n",
    "\n",
    "            \n",
    "            \n",
    "stroke_svg_df= pd.DataFrame.from_dict(stroke_svgs, orient='index')    \n",
    "stroke_group_data= D.groupby(['stroke_id']).agg(lambda x:x.value_counts().index[0])\n",
    "labels= pd.DataFrame(stroke_group_data[['sketch_id','label','stroke_num','condition','target','category']])\n",
    "stroke_df=pd.merge(stroke_svg_df,labels,left_index=True, right_index =True)\n",
    "stroke_df.reset_index(level=0, inplace=True)\n",
    "stroke_df=stroke_df.rename(index=str, columns={\"index\": \"stroke_id\", 0: \"svg\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Setting up for building transition matrices for each different category\n",
    "\n",
    "from itertools import product as p\n",
    "\n",
    "tm_dict={}\n",
    "for category in unique_cats:\n",
    "    stroke_df_s = stroke_df[stroke_df['category']==category]  \n",
    "    num_uniq_labs = len(np.unique(stroke_df_s['label']))\n",
    "    temp_array = np.zeros([num_uniq_labs*num_uniq_labs,2],dtype='|S50')\n",
    "    ind=0\n",
    "    for roll in p(stroke_df_s['label'].unique().tolist(), repeat = 2):\n",
    "        temp_array[ind,]= roll\n",
    "        ind+=1\n",
    "    for sketch in stroke_df_s.sketch_id.unique():\n",
    "        sketch_df= stroke_df_s[stroke_df_s.sketch_id==sketch]\n",
    "        sketch_df['incr_stroke_num'] = sketch_df['stroke_num']+1\n",
    "        tm_df=sketch_df.merge(sketch_df, right_on='stroke_num', left_on='incr_stroke_num', how='inner' )\n",
    "        plot_matrix_x= tm_df.label_x.append(pd.Series(temp_array[:,0]))\n",
    "        plot_matrix_y=tm_df.label_y.append(pd.Series(temp_array[:,1]))\n",
    "        ct_df= pd.crosstab(plot_matrix_x, plot_matrix_y)-1\n",
    "        #ct_df= pd.crosstab(tm_df.label_x, tm_df.label_y)\n",
    "        mat= np.matrix(ct_df).sum()\n",
    "        plot_df = ct_df.div(mat, axis=0).round(2)\n",
    "        plot_df=plot_df.fillna(0)\n",
    "        tm_dict[sketch]= plot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in unique_cats:\n",
    "    cat_matrices=[]\n",
    "    DS=D[D['category']==category]\n",
    "    for sketch in DS['sketch_id'].unique():\n",
    "        cat_matrices.append(tm_dict[sketch])\n",
    "    fig,ax=plt.subplots(1, 1, figsize = (10, 8), dpi=150)\n",
    "    agg_matrix= sum(cat_matrices)\n",
    "    divider=np.matrix(agg_matrix).sum()\n",
    "    agg_matrix = agg_matrix.div(divider, axis=0).round(2)\n",
    "    sns.heatmap(agg_matrix,cmap=\"YlGnBu\", annot=True)\n",
    "    plt.title(\"Part transition matrix for {}\".format(category))\n",
    "    ax.set_ylabel('')    \n",
    "    ax.set_xlabel('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at stroke  variation between parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Investigating how stroke length varies between parts given a category of objects \n",
    "\n",
    "for category in unique_cats:\n",
    "    DS= D[D['category']==category]\n",
    "    num_unique_labs = len(DS['label'].unique())\n",
    "    strokes_in_part_vect = np.zeros((len(np.unique(DS['annotation_id']))*num_unique_labs,3), dtype='|a1000')\n",
    "    ind=0\n",
    "    for this_annotation in np.unique(DS['annotation_id']):    \n",
    "        DSA= DS[DS['annotation_id']==this_annotation]\n",
    "        for this_label in np.unique(DS['label']):\n",
    "            DSB=DSA[DSA['label']==this_label]\n",
    "            strokes_in_part_vect[ind,]=[this_annotation, this_label,len(np.unique(DSB['stroke_num']))]\n",
    "            ind+=1\n",
    "    strokes_in_part_vect=strokes_in_part_vect[~np.all(strokes_in_part_vect == '', axis=1)]\n",
    "    strokes_in_part_df= pd.DataFrame(strokes_in_part_vect, columns=['annotation_id','part','num_strokes'])\n",
    "    strokes_in_part_df['num_strokes']=pd.to_numeric(strokes_in_part_df['num_strokes'])\n",
    "    fig,ax=plt.subplots(1, 1, figsize = (10, 8), dpi=150)\n",
    "    sns.barplot(x='part',y='num_strokes',data=strokes_in_part_df,ci=95,capsize=0.3, errwidth= 3)\n",
    "    plt.title('Average number of strokes for {} parts'.format(category))\n",
    "    plt.ylabel('Number of strokes')\n",
    "    plt.xlabel('Part')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svgpathtools import parse_path\n",
    "import svgpathtools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df['arc_length'] = \"\"\n",
    "for s,stroke in stroke_df.iterrows():\n",
    "    stroke_df['arc_length'][s] = parse_path(stroke['svg']).length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Investigating how arc length varies between parts\n",
    "\n",
    "for category in unique_cats:\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.barplot(x='label', y='arc_length', data=stroke_df[stroke_df['category']=='dog'], ci=68,capsize=0.3, errwidth= 3)\n",
    "    plt.title('Average stroke lengths for {} parts'.format(category))\n",
    "    plt.ylabel('arc length')\n",
    "    plt.xlabel('part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Investigating how arc length varies between parts x conditions\n",
    "\n",
    "for category in unique_cats:\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.barplot(x='label',y='arc_length',hue='condition', data=stroke_df[stroke_df['category']==category],ci=68,capsize=0.3, errwidth= 3)\n",
    "    plt.title('Average stroke lengths for {} parts'.format(category))\n",
    "    plt.ylabel('Arc length')\n",
    "    plt.xlabel('Part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Might Delete this - calculating average arc length per part manually####\n",
    "part_lengths={}\n",
    "for category in unique_cats:\n",
    "    stroke_df_lite= stroke_df[stroke_df['category']==category]\n",
    "    arc_length_dict = OrderedDict()\n",
    "    for label in np.unique(stroke_df_lite['label']):\n",
    "        stroke_df_lite0= stroke_df_lite[stroke_df_lite['label']==label]\n",
    "        path_length=0\n",
    "        for stroke in np.unique(stroke_df_lite0['stroke_id']):\n",
    "            path = parse_path(stroke_df_lite0[stroke_df_lite['stroke_id']==stroke]['svg'].iloc[0])\n",
    "            path_length= path_length+path.length()\n",
    "        arc_length_dict[label]= path_length/len(stroke_df_lite0)\n",
    "    part_lengths[category]=arc_length_dict\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at annotation bouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Investigating the number of parts for each part within a category. Does this number correspond to number \n",
    "###of strokes with that part label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D['part_bout_num'] = pd.to_numeric(D['part_bout_num'])\n",
    "\n",
    "for category in unique_cats:\n",
    "    DS=D[D['category']==category]\n",
    "    unique_labels_in_cat= DS['label'].unique()\n",
    "    plot_vect = np.zeros([1,2], dtype='|a20')\n",
    "    for label in unique_labels_in_cat:\n",
    "        DSA=DS[DS['label']==label]\n",
    "        total_bouts=0\n",
    "        for sketch in DSA['sketch_id'].unique():\n",
    "            DSB= DSA[DSA['sketch_id']==sketch]\n",
    "            num_bouts = len(DSB['part_bout_num'].unique())\n",
    "            temp= np.array([label, num_bouts], dtype= '|a20')\n",
    "            plot_vect=np.vstack((plot_vect, temp))\n",
    "    plot_vect=np.delete(plot_vect, (0), axis=0)  \n",
    "    plot_df= pd.DataFrame(plot_vect, columns=['label','num_bouts'])\n",
    "    plot_df['num_bouts']=pd.to_numeric(plot_df['num_bouts'])\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.barplot(x='label',y='num_bouts',data= plot_df,capsize=0.3)\n",
    "    plt.title('Average bout numbers for {} parts'.format(category))\n",
    "    plt.ylabel('Average Number of Bouts')\n",
    "    plt.xlabel('Part')\n",
    "\n",
    "        #label_group_data=DS.groupby('label').agg({'part_bout_num':\"mean\"})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
