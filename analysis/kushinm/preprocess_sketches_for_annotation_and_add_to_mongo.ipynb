{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib as mp\n",
    "from matplotlib import pyplot,pylab\n",
    "plt = pyplot\n",
    "import scipy\n",
    "from __future__ import division\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "import string\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "\n",
    "import json\n",
    "import pymongo as pm\n",
    "\n",
    "from svgpathtools import parse_path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload sketches to S3 [maybe do this later]\n",
    "#### build stimulus dictionary and write to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload sketches to S3 [todo later]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build stimulus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read in experimental metadata file (CSV)\n",
    "path_to_metadata = 'sketchpad_basic_allcats.csv'\n",
    "_meta = pd.read_csv(path_to_metadata)   ### raw meta with all categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print first few rows of meta\n",
    "_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: what we actually want Make sure that the data we're excluding from annotation \n",
    "## really the come from the trials where we had the shift-key artifact. \n",
    "## Right now, we are excluding sketches with numStrokes > mu + 3*sd, which is an imperfect proxy for that.\n",
    "mu = np.mean(_meta['numStrokes'])\n",
    "sd = np.std(_meta['numStrokes'])\n",
    "thresh = mu + 3*sd\n",
    "_meta = _meta[_meta['numStrokes']<thresh]\n",
    "_meta.reset_index(inplace=True) ## reset index on meta_chairs\n",
    "\n",
    "## subset by chairs\n",
    "meta_chairs = _meta[_meta['category']=='chair'] ### subsetted meta with just chairs\n",
    "\n",
    "## sub-select good 4 chairs\n",
    "## inlay>waiting>straight>leather\n",
    "chairs4_list = ['inlay','waiting','straight','leather']\n",
    "meta_chairs4 = meta_chairs[meta_chairs['target'].isin(chairs4_list)]\n",
    "meta_chairs4.reset_index(inplace=True)\n",
    "\n",
    "## assign which meta we will actually upload to mongo in this session\n",
    "category_flag = 'allcats' ## options: ['allcats','chairs','chairs4']\n",
    "\n",
    "if category_flag == 'chairs':\n",
    "    meta = meta_chairs\n",
    "elif category_flag == 'chairs4':\n",
    "    meta = meta_chairs4\n",
    "elif category_flag == 'allcats':\n",
    "    meta = _meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add parts list\n",
    "parts =[]\n",
    "for i in range(meta.shape[0]-1):\n",
    "    if meta.category[i]==\"chair\":\n",
    "        parts.append([\"backrest,armrest,seat,leg\"])        \n",
    "    if meta.category[i]==\"dog\":\n",
    "        parts.append([\"eye,mouth,ear,head,neck,body,leg,paw,tail\"])\n",
    "    if meta.category[i] == \"bird\":\n",
    "        parts.append([\"eye,beak,head,body,wing,leg,feet,tail\"])\n",
    "    if meta.category[i] == \"car\":\n",
    "        parts.append([\"bumper,headlight,hood,windshield,window,body,door,trunk,wheel\"])\n",
    "meta = meta.assign(parts=pd.Series(parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add iteration name information\n",
    "_iterationName = 'sketchpad_basic_{}'.format(category_flag)\n",
    "iterationName = [_iterationName]*len(meta)\n",
    "meta = meta.assign(iterationName=pd.Series(iterationName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## svg string formatting\n",
    "svg = []\n",
    "for i,d in meta.iterrows():    \n",
    "    splitted = d['svg'].split(\"'\") ## parse string to re-split up into strokes\n",
    "    svgString = [i for i in splitted if i[0]=='M'] ## check to make sure it is a real start of a spline\n",
    "    svg.append(svgString)\n",
    "meta = meta.assign(svg=pd.Series(svg)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add numSplines to the meta data\n",
    "numSplines = []\n",
    "for sk_ind, sketch in enumerate(meta.svg.values):\n",
    "    num_splines = 0\n",
    "    for stroke_ind,stroke in enumerate(sketch):\n",
    "        parsed = parse_path(stroke)\n",
    "        num_splines += len(parsed)\n",
    "    numSplines.append(num_splines)\n",
    "meta = meta.assign(numSplines=pd.Series(numSplines))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add empty games list\n",
    "games = [[] for i in np.arange(len(meta))]\n",
    "meta = meta.assign(games=pd.Series(games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## filter so we can cover all the sketches multiple times with a budget of around $500 \n",
    "budget_cap = 500\n",
    "base_pay = 1.00\n",
    "spline_bonus = 0.002\n",
    "completion_bonus = 0.02 \n",
    "expected_bonus = 0.\n",
    "amt_commission = 1.2\n",
    "mean_cost_single_session = (base_pay + expected_bonus) * amt_commission\n",
    "print 'We expect to pay out approx. ${:.2f} per session, including commission.'.format(mean_cost_single_session)\n",
    "num_sessions = int(np.floor(budget_cap/mean_cost_single_session))\n",
    "print 'We have enough money to run approx. {} sessions.'.format(num_sessions)\n",
    "num_total_annotation_trials_in_budget = num_sessions * 10\n",
    "num_times_per_sketch = 3\n",
    "num_unique_sketches_annotatable = num_total_annotation_trials_in_budget/3\n",
    "print 'We can afford to annotate approx. {} sketches {} times each.'.format(int(num_unique_sketches_annotatable), num_times_per_sketch)\n",
    "num_sketches_per_game = 32\n",
    "print \"That means we can fully annotate approx. {} games' worth of sketches.\".format(int(num_unique_sketches_annotatable/num_sketches_per_game))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(np.unique(meta['gameID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### maybe filter on games that had both high accuracy and big context effect (big diff in num strokes between close & far)\n",
    "## get list of games sorted by accuracy\n",
    "games_sorted_by_accuracy = meta.groupby(['gameID'])['outcome'].mean().reset_index().sort_values('outcome',ascending=False).gameID.values\n",
    "\n",
    "## get list of games sorted by how many more strokes were used in close condition than far condition\n",
    "# reshape dataframe that computes mean strokes for each condition for each game\n",
    "mean_strokes_in_condition_per_game = meta.groupby(['gameID','condition'])['numStrokes'].mean().reset_index().sort_values('gameID',ascending=True)\n",
    "mscg = mean_strokes_in_condition_per_game\n",
    "# pivots table to make two columns for each condition\n",
    "msg = mscg.pivot(index='gameID', columns='condition')['numStrokes'].reset_index()\n",
    "# assigns new column with close vs. far diff, then sorts\n",
    "msg['diff'] = msg.apply(lambda x: x['closer'] - x['further'],axis=1)\n",
    "msg = msg.sort_values('diff',ascending=False).reset_index()\n",
    "# get list of games sorted by difference in num strokes used in each condition\n",
    "games_sorted_by_contextDiff = msg['gameID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## come up with some way of filtering for games that had both relatively high accuracy and strong context effect...\n",
    "game_list = games_sorted_by_accuracy\n",
    "acc_rank = []\n",
    "con_rank = []\n",
    "composite_rank = []\n",
    "for this_game in game_list:\n",
    "    gsa = games_sorted_by_accuracy\n",
    "    gsc = games_sorted_by_contextDiff\n",
    "\n",
    "    a = np.where(gsa==this_game)[0][0] # rank of this game in accuracy-ranked list\n",
    "    b = np.where(gsc==this_game)[0][0] # rank of this game in contextDiff-ranked list\n",
    "    c = a + b # composite rank score\n",
    "    acc_rank.append(a)\n",
    "    con_rank.append(b)\n",
    "    composite_rank.append(c)\n",
    "    \n",
    "## plot acc_rank against con_rank -- are they related at all?\n",
    "plt.scatter(acc_rank,con_rank)\n",
    "stats.pearsonr(acc_rank,con_rank)\n",
    "plt.xlabel('rank by accuracy')\n",
    "plt.ylabel('rank by context diff')\n",
    "thresh = 70\n",
    "plt.plot([0,thresh],[thresh,0],'k:')\n",
    "plt.xlim(0,93)\n",
    "plt.ylim(0,93)\n",
    "plt.show()  \n",
    "print 'There were {} games that had composite rank score < {}.'.format(len([i for i in composite_rank if i<thresh]),thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Choose a filter mode: accuracy or composite\n",
    "filter_mode = 'accuracy'\n",
    "\n",
    "if filter_mode == 'composite':\n",
    "##extract the list of \"good games\" with lowest composite score\n",
    "    good_games = [i for (i,j) in zip(game_list,composite_rank) if j<70]\n",
    "elif filter_mode == 'accuracy':\n",
    "##extract the list of \"good games\" with lowest accuracy rank   \n",
    "    good_games = [i for (i,j) in zip(game_list, acc_rank) if j<50]\n",
    "\n",
    "## now define a meta2 dataframe that ONLY contains data from the \"good games\"\n",
    "meta2 = meta[meta['gameID'].isin(good_games)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## toggle whether we want to use the full dataset or the subset\n",
    "subsetted = True\n",
    "if subsetted:\n",
    "    meta = meta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## how many games worth of data do we have?\n",
    "print '{} unique games worth of data.'.format(len(np.unique(meta.gameID.values)))\n",
    "print '{} unique sketches.'.format(len(meta))\n",
    "\n",
    "## write out metadata to json file\n",
    "\n",
    "## for example:\n",
    "stimdict = meta2.to_dict(orient='records')\n",
    "stimdict\n",
    "import json\n",
    "with open('annotation_meta_{}.js'.format(category_flag), 'w') as fout:\n",
    "     json.dump(stimdict, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interlude to examine detailed statistics on constituent splines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## svg is a list of sketches\n",
    "## each entry contains a list of strokes\n",
    "## first let's convert into absolute coordinates\n",
    "## then let's convert these into a list of splines that are \"long enough\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_relative_spline_to_absolute(parsed):\n",
    "    svg_abs = ''\n",
    "    for i,p in enumerate(parsed):\n",
    "        if len(p)==4: ## cubic bezier\n",
    "            svg_abs += ' M '\n",
    "            svg_abs += '{},{}'.format(str(p.start.real),str(p.start.imag))\n",
    "            svg_abs += ' C'\n",
    "            svg_abs += ' {},{}'.format(str(p.control1.real),str(p.control1.imag))\n",
    "            svg_abs += ' {},{}'.format(str(p.control2.real),str(p.control2.imag))\n",
    "            svg_abs += ' {},{}'.format(str(p.end.real),str(p.end.imag)) \n",
    "        if len(p)==2: ## line segment\n",
    "            svg_abs += ' M '\n",
    "            svg_abs += '{},{}'.format(str(p.start.real),str(p.start.imag))\n",
    "            svg_abs += ' L'\n",
    "            svg_abs += ' {},{}'.format(str(p.end.real),str(p.end.imag))          \n",
    "#     assert np.all(np.round(parsed)==np.round(parse_path(svg_abs)))==True\n",
    "    return svg_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get list of sketch svg converted to absolute coordinates\n",
    "## grouped into **strokes**\n",
    "svg_abs_strokes = []\n",
    "for this_sketch in svg:\n",
    "    sketch_abs = []\n",
    "    for this_stroke in this_sketch:  \n",
    "        this_stroke = this_stroke.replace('v0','') ## eliminate single points\n",
    "        this_stroke = this_stroke.replace('h0','') ## eliminate single points\n",
    "        parsed = parse_path(this_stroke)\n",
    "        parsed_abs = convert_relative_spline_to_absolute(parsed)\n",
    "        sketch_abs.append(parsed_abs)\n",
    "    svg_abs_strokes.append(sketch_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get list of sketch svg converted to absolute coordinates\n",
    "## grouped into **splines**\n",
    "svg_abs_splines = [] \n",
    "stroke_num_within_sketch = [] ## get stroke num within sketch\n",
    "for this_sketch in svg_abs_strokes:\n",
    "    sketch_abs = []\n",
    "    _stroke_num_within_sketch = []    \n",
    "    for stroke_id,this_stroke in enumerate(this_sketch):\n",
    "        this_path = parse_path(this_stroke)\n",
    "        for i,p in enumerate(this_path):\n",
    "            _svg_abs = ''\n",
    "            if len(p)==4: ## cubic bezier\n",
    "                _svg_abs += ' M '\n",
    "                _svg_abs += '{},{}'.format(str(p.start.real),str(p.start.imag))\n",
    "                _svg_abs += ' C'\n",
    "                _svg_abs += ' {},{}'.format(str(p.control1.real),str(p.control1.imag))\n",
    "                _svg_abs += ' {},{}'.format(str(p.control2.real),str(p.control2.imag))\n",
    "                _svg_abs += ' {},{}'.format(str(p.end.real),str(p.end.imag)) \n",
    "            if len(p)==2: ## line segment\n",
    "                _svg_abs += ' M '\n",
    "                _svg_abs += '{},{}'.format(str(p.start.real),str(p.start.imag))\n",
    "                _svg_abs += ' L'\n",
    "                _svg_abs += ' {},{}'.format(str(p.end.real),str(p.end.imag))  \n",
    "            sketch_abs.append(_svg_abs)\n",
    "            _stroke_num_within_sketch.append(stroke_id)\n",
    "    svg_abs_splines.append(sketch_abs)\n",
    "    stroke_num_within_sketch.append(_stroke_num_within_sketch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create list of spline arc lengths nested in the same way as svg_abs_splines\n",
    "svg_abs_spline_lengths = []\n",
    "for sketch_ind,this_sketch in enumerate(svg_abs_splines):\n",
    "    sketch_abs_length = []\n",
    "    for spline_ind,spline in enumerate(this_sketch):\n",
    "        curr_stroke_ind = stroke_num_within_sketch[sketch_ind][spline_ind]    \n",
    "        curr_spline_length = parse_path(spline).length()\n",
    "        sketch_abs_length.append(curr_spline_length)\n",
    "    svg_abs_spline_lengths.append(sketch_abs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    return [item for sublist in x for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get all spline lengths\n",
    "flat_spline_lengths = flatten(svg_abs_spline_lengths)\n",
    "\n",
    "## make figure\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.set_context('paper')\n",
    "plt.title('Distribution of spline arc lengths across all sketches')\n",
    "plt.hist(flat_spline_lengths,200)\n",
    "plt.xticks(np.linspace(0,200,51))\n",
    "plt.xlim(0,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload stim dictionary to mongo (db = 'stimuli', collection='sketchpad_basic_recog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## do you want to upload a \"development mode\" collection for testing?\n",
    "dev_mode = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load in the JSON that contains the svgData, object labels, and part labels\n",
    "J = json.loads(open('annotation_meta_{}.js'.format(category_flag),mode='ru').read())\n",
    "assert len(J)==len(meta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv('auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'rxdhawkins.me' ## cocolab ip address\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define the dbname and collection name\n",
    "db = conn['stimuli']\n",
    "if dev_mode:\n",
    "    coll = db['svg_annotation_sketchpad_basic_{}_dev'.format(category_flag)]\n",
    "else:\n",
    "    coll = db['svg_annotation_sketchpad_basic_{}'.format(category_flag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## actually add data now to the database (iff reallyRun==True)\n",
    "reallyRun = True\n",
    "if reallyRun:\n",
    "    for (i,j) in enumerate(J):\n",
    "        if i%250==0:\n",
    "            print ('%d of %d' % (i,len(J)))\n",
    "        coll.insert_one(j)\n",
    "reallyRun = False        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## How many sketches do we have in the database?\n",
    "print 'We have {} sketches.'.format(coll.count())\n",
    "\n",
    "## What kind of sketches do we have in the database?\n",
    "print 'We have these kinds: {}'.format(str(coll.distinct('category')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### check how many sketches have been tagged in the collection how many times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cumulative = []\n",
    "print 'There are: '\n",
    "for i in np.arange(10):\n",
    "    print '{} sketches that have been retrieved {} times from the database.'.format(coll.find({'games': {'$size': i}}).count(),i)\n",
    "    cumulative.append(coll.find({'games': {'$size': i}}).count())\n",
    "print 'While not guaranteed, sketches retrieved from the database are usually annotated, so this is a reasonable proxy for how many sketches have been annotated.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
